{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 4 (Optional)\n",
    "\n",
    "## 1-3(Done), 4(not Done), 5(not Done), 6(not Done) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, write your implementation within the designated blocks:\n",
    "```python\n",
    "...\n",
    "### BEGIN Solution\n",
    "\n",
    "# >>> your solution here <<<\n",
    "\n",
    "### END Solution\n",
    "...\n",
    "```\n",
    "\n",
    "Write your theoretical derivations within such blocks:\n",
    "```markdown\n",
    "**BEGIN Solution**\n",
    "\n",
    "<!-- >>> your derivation here <<< -->\n",
    "\n",
    "**END Solution**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly / outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will need to work through a modified version of\n",
    "the SVDD model (**support vector data description**) -- a model for outlier\n",
    "detection, and show some theoretical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we have a dataset $x_1, \\ldots, x_l$ from some set $\\mathcal{X}$.\n",
    "\n",
    "We apply the kernel trick using the kernel $K \\colon \\mathcal{X} \\times \\mathcal{X}\n",
    "\\to \\mathbb{R}$ of the Hilbert space $\\bigl(\\mathcal{H}, \\langle \\cdot,\n",
    "\\cdot \\rangle\\big)$ with the feature mapping $\\phi(\\cdot)\\colon \\mathcal{X}\n",
    "\\to \\mathcal{H}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that $\\nu \\in (0, 1)$, and $C_i > 0$ with $\\sum_{i=1}^l C_i = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVDD model (kernelized) is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\underset{R, h, \\xi}{\\text{minimize}}\n",
    "      & & R + \\frac1\\nu \\sum_{i=1}^l C_i \\xi_i\n",
    "          \\,, \\\\\n",
    "    & \\text{subject to}\n",
    "      & & \\|\\phi(x_i) - h \\|^2 \\leq R + \\xi_i\n",
    "          \\,, \\\\\n",
    "    & & & \\xi_i \\geq 0\n",
    "          \\,.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (3 pt.): Can $R$ be negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the SVDD problem with an extra constraint $R \\geq 0$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\underset{R, h, \\xi}{\\text{minimize}}\n",
    "      & & R + \\frac1\\nu \\sum_{i=1}^l C_i \\xi_i\n",
    "          \\,, \\\\\n",
    "    & \\text{subject to}\n",
    "      & & \\|\\phi(x_i) - h \\|^2 \\leq R + \\xi_i\n",
    "          \\,, \\\\\n",
    "    & & & \\xi_i \\geq 0\n",
    "          \\,, \\\\\n",
    "    & & & R \\geq 0\n",
    "          \\,.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R \\geq 0$ constraint is a nuisance.\n",
    "\n",
    "* Show, that if $(R, \\xi, h)$ has $R < 0$, then it **is not better** than a\n",
    "certain other feasible candidate $(\\hat{R}, \\hat{\\xi}, \\hat{h})$ with $\\hat{R} \\geq 0$.\n",
    "* Argue that it is, in fact, **redundant**, i.e. the problem formulations\n",
    "**with it** and **without it** have identical solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. Let's look on objective functions for $(R < 0, \\xi, h)$. Thus, $|R| = -R \\rightarrow R = -|R|$\n",
    "$$\n",
    "-|R| + \\frac{1}{\\nu} \\sum_{i = 1}^{l} C_i \\xi_i \\geq -|R| + \\frac{1}{\\nu} \\sum_{i = 1}^{l} C_i (\\|\\phi(x_i) - h \\|^2 + |R|)$$\n",
    "$$ -|R| + \\frac{1}{\\nu} \\sum_{i = 1}^{l} C_i (\\|\\phi(x_i) - h \\|^2 + |R|)= \\frac{1}{\\nu}|R| - |R| + \\frac{1}{\\nu}\\sum_{i = 1}^{l} C_i \\|\\phi(x_i) - h \\|^2$$ \n",
    "$$ \\frac{1}{\\nu}|R| - |R| + \\frac{1}{\\nu}\\sum_{i = 1}^{l} C_i \\|\\phi(x_i) - h \\|^2> \\frac{1}{\\nu} \\sum_{i = 1}^{l} C_i \\|\\phi(x_i) - h \\|^2\n",
    "$$\n",
    " 2. Now we can finally write for objective function:\n",
    "$$-|R| + \\frac{1}{\\nu} \\sum_{i = 1}^{l} C_i \\xi_i  > \\frac{1}{\\nu} \\sum_{i = 1}^{l} C_i \\|\\phi(x_i) - h \\|^2$$\n",
    " 3. Let's look the other feasible candidate with $(\\hat{R} = 0, \\hat{\\xi}, \\hat{h} = h)$ with subject to $\\|\\phi(x_i) - h \\|^2 = \\hat{\\xi}_i$\n",
    "$$\n",
    "\\frac{1}{\\nu} \\sum_{i = 1}^{l} C_i \\hat{\\xi}_i = \\frac{1}{\\nu} \\sum_{i = 1}^{l} C_i \\hat{\\xi}_i = \\frac{1}{\\nu} \\sum_{i = 1}^{l} C_i \\|\\phi(x_i) - h\\|^2  \n",
    "$$\n",
    " 4. From this and previous inequality we can see, that for some feasible candidate with $\\hat{R} \\geq 0$ we receive solution which is to be better, than the optimal one for $R < 0$. It's not guaranteed that the represented solution in second point to be optimal. Thus, we proved the first point of the task.\n",
    "\n",
    "* For $R \\in \\mathbb{R}$ condition $R \\geq 0$ is redundant, because for optimal solution will have $R \\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (2 pt.): Can $\\xi_i > 0$ for all $i$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argue if $(R, \\xi, h)$ is a solution, then $\\xi_i = 0$ for at least one $i=1,\\,\\ldots,\\,l$. Let $\\bar{\\xi} = \\min_{j=1}^l \\xi_j$.\n",
    "\n",
    "**HINT** Use an argument similar to Task $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Suppose, we have solution $(R, \\xi, h)$. Let's denote as $k =  \\text{arg}\\min_{j=1}^l \\xi_j$ - index of minimal $\\xi_i$.\n",
    "2. Imagine for $\\forall i \\ \\xi_i > 0 \\rightarrow \\bar{\\xi} >0$. \n",
    "3. So for our objective function (let's denote as $\\mathcal{F}$), subject to $\\bar{\\xi} >0$, using denotation $C_i = |i = k| = \\bar{C}$:\n",
    "$$\n",
    "\\mathcal{F} = R + \\frac{1}{\\nu}\\sum_{i=1}^l C_i \\xi \n",
    "$$\n",
    "We can substract minimal $\\xi_i$ from sum:\n",
    "$$\n",
    "\\mathcal{F} = R + \\frac{1}{\\nu}\\bar{C}\\bar{\\xi} +  \\frac{1}{\\nu} \\sum_{i=1,i\\neq k}^l C_i \\xi \n",
    "$$\n",
    "4. Let's choose another point $(\\hat{R}, \\hat{\\xi}, h)$, such that:\n",
    "$$\\hat{R} = R + \\bar{\\xi}, \\hat{\\xi} = \\xi - \\bar{\\xi}$$\n",
    "where last equation must be interpreted as character subtraction.\n",
    "5. Point from 4. satisfies all given above conditions and minimal value for $\\hat{\\xi}_i = 0$.\n",
    "6. So, for objective function (let's denote as $\\hat{\\mathcal{F}}$):\n",
    "$$\n",
    "\\hat{\\mathcal{F}} = \\hat{R} + \\frac{1}{\\nu} \\sum_{i=1}^l C_i \\hat{\\xi}\n",
    "$$\n",
    "or using denotation for $(\\hat{R}, \\hat{\\xi}, h)$\n",
    "$$\n",
    "\\hat{\\mathcal{F}} = R + \\bar{\\xi} + \\frac{1}{\\nu} \\sum_{i=1}^l C_i (\\xi - \\bar{\\xi})\n",
    "$$\n",
    "7. Using that fact that minimal value for $\\xi_i = 0$ (denote index as $k$), we obtain:\n",
    "$$\n",
    "\\hat{\\mathcal{F}} = R + \\bar{\\xi} + \\frac{1}{\\nu} \\sum_{i=1, i \\neq k}^l C_i (\\xi - \\bar{\\xi}) =  R + \\bar{\\xi}(1 - \\frac{1}{\\nu} \\sum_{i=1, i \\neq k}^l C_i ) + \\frac{1}{\\nu}  \\sum_{i=1, i \\neq k}^l C_i \\xi\n",
    "$$\n",
    "$$\n",
    "\\hat{\\mathcal{F}}  =  R + \\bar{\\xi} +  \\frac{1}{\\nu}  \\sum_{i=1, i \\neq k}^l C_i \\xi - \\frac{1}{\\nu}(1 - \\bar{C})\\bar{\\xi} = \\mathcal{F} + (\\frac{1}{\\nu})\\bar{\\xi} < \\mathcal{F}\n",
    "$$\n",
    "8. From 7. we see, that $\\forall$ points with $\\bar{\\xi} > 0$ we can decrease value of objective function, by choosing new point with $\\bar{\\xi} = 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (2 pt.): The Lagrangian and KKT conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, write out the Lagrangian function of the problem and write out the KKT conditions\n",
    "* Lagrangian\n",
    "* the first order conditions\n",
    "* the complementary slackness conditions\n",
    "* the primal and dual constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Langrangian $\\mathcal{L}$: \n",
    "$$\n",
    "\\mathcal{L} = R + \\frac{1}{\\nu}\\sum_{i = 1}^l  C_i\\xi_i  - \\sum_{i = 1}^l\\alpha_i(R + \\xi_i - \\|\\phi(x_i) - h \\|^2) - \\sum_{i = 1}^l \\beta_i \\xi_i\n",
    "$$\n",
    "* And KKT conditions:\n",
    "   1. First order conditions: \n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial \\xi_i} = \\frac{1}{\\nu}C_i - \\alpha_i = \\beta_i \\rightarrow \\frac{1}{\\nu}C_i - \\beta_i - \\alpha_i,  i = \\overline{1,l}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial R} = 1 - \\sum_{i=1}^{l} \\alpha_i = 0 \\rightarrow \\sum_{i=1}^{l} \\alpha_i = 1\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial h} = \\sum_{i = 1}^{l} 2 \\alpha_i (\\phi(x_i) - h) = 0 \\rightarrow \\sum_{i = 1}^{l} 2 \\alpha_i \\phi(x_i)  - \\sum_{i = 1}^{l} 2 \\alpha_i h = 0 \\rightarrow \\sum_{i = 1}^{l}  \\alpha_i \\phi(x_i) - h \\sum_{i = 1}^{l} \\alpha_i = 0 \n",
    "   $$\n",
    "   Using first order condition above:\n",
    "   $$\n",
    "   h =  \\sum_{i = 1}^{l}  \\alpha_i \\phi(x_i)\n",
    "   $$\n",
    "   2. The complementary slackness conditions:\n",
    "   $$\n",
    "   \\alpha_i(R + \\xi_i = \\|\\phi(x_i) - h\\|^2) = 0 , i = \\overline{1,l}\\\\\n",
    "   $$\n",
    "   $$\n",
    "   \\beta_i \\xi_i = 0, i = \\overline{1,l}\n",
    "   $$\n",
    "   3. The primal and dual constrains: \n",
    "   $$\n",
    "   \\alpha_i \\geq 0, \\beta_i \\geq 0, i = \\overline{1,l} \\\\\n",
    "   $$\n",
    "   $$\n",
    "   R + \\xi_i - \\|\\phi(x_i) - h\\|^2 \\geq 0, \\xi_i \\geq 0\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (2 pt.): Simplifying the Lagrangian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down $h$ as a function of transformed input data and dual coefficients,\n",
    "and using the first order conditions rewrite the Lagrangian in such a way, that\n",
    "it only contains dual variables and evaluations of the kernel $K(\\cdot, \\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Expression for $h$ was written in previous task. Thus for the Lagrangian:\n",
    "$$\n",
    "\\mathcal{L} = R + \\frac{1}{\\nu}\\sum_{i = 1}^l  C_i\\xi_i  - \\sum_{i = 1}^l\\alpha_i(R + \\xi_i - \\|\\phi(x_i) - h \\|^2) - \\sum_{i = 1}^l \\beta_i \\xi_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Consider last condition:\n",
    "$$\n",
    "\\sum_{i = 1}^{l} 2 \\alpha_i (\\phi(x_i) - h) = 0 \\rightarrow h = \\frac{\\sum_{i = 1}^{l} \\alpha_i\\phi(x_i)}{\\sum_{i = 1}^{l} \\alpha_i } = \\sum_{i = 1}^{l} \\alpha_i\\phi(x_i)\n",
    "$$\n",
    "Now let's find simple from of Langrangian:\n",
    "$$\n",
    " L(R, \\xi, h , \\alpha_i, \\beta_i) = R + \\frac{1}{\\nu} C_i\\xi_i - \\sum_{i = 1}^l \\beta_i \\xi_i  - \\sum_{i = 1}^l\\alpha_i(R + \\xi_i - \\|\\phi(x_i) - h \\|^2) = R + \\frac{1}{\\nu} \\sum_{i = 1}^l  C_i\\xi_i - \\sum_{i = 1}^l\\alpha_i R - \\sum_{i = 1}^l\\alpha_i \\xi_i + \\sum_{i = 1}^{l} \\alpha_i \\|\\phi(x_i) - h \\|^2  = \\\\  \n",
    " =\\sum_{i = 1}^{l} \\alpha_i \\|\\phi(x_i) - h \\|^2 +  \\sum_{i = 1}^l( \\frac{1}{\\nu} C_i - \\alpha_i) \\xi_i = \\sum_{i = 1}^{l} \\alpha_i \\|\\phi(x_i) - h \\|^2 + \\sum_{i = 1}^l\\beta_i \\xi_i = \\\\\n",
    " =\\sum_{i = 1}^{l} \\alpha_i \\|\\phi(x_i) - h \\|^2 = \\sum_{i = 1}^{l} \\alpha_i (\\langle \\phi(x_i);\\phi(x_i) \\rangle - 2 \\langle h;\\phi(x_i) \\rangle + \\langle h;h\\rangle) = \\\\\n",
    " = \\sum_{i = 1}^{l}\\alpha_i K(x_i;x_i) - 2\\sum_{i,j = 1}^{l} \\alpha_i \\alpha_jK(x_i;x_j) + \\sum_{i,j = 1}^{l}\\alpha_i \\alpha_j K(x_i;x_j) = \\sum_{i = 1}^{l}\\alpha_i K(x_i;x_i) - \\sum_{i,j = 1}^{l}\\alpha_i \\alpha_j K(x_i;x_j)\n",
    "$$\n",
    "Thus we have :\n",
    "$$\n",
    "L(R, \\xi, h , \\alpha_i, \\beta_i) = \\sum_{i = 1}^{l}\\alpha_i K(x_i;x_i) - \\sum_{i,j = 1}^{l} \\alpha_i \\alpha_jK(x_i;x_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 (2 pt.): The dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Carefully eliminate $\\beta_i$ from the KKT conditions, and write\n",
    "down the inequality constraints for the dual variables $\\alpha_i$,\n",
    "implied by the FOC.\n",
    "\n",
    "* Combine your results into dual problem (minimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, due to $\\alpha_i \\geq 0$, $\\frac{1}{\\nu}C_i - \\alpha_i - \\beta_i = 0$ and $\\beta_i \\geq 0$, we have $0 \\leq \\alpha_i \\leq \\frac{1}{\\nu}C_i$. All other conditions except $\\sum_{i=1}^l \\alpha_i = 1$ is already satisfied by our derivation of simple form of Langrangian. Thus, we have optimization problem:\n",
    "$$\n",
    "\\min \\sum_{i,j = 0}^{l} K(x_i;x_j) - \\sum_{i = 1}^{l}\\alpha_i K(x_i;x_i) \\\\\n",
    "\\text{s.t} \\sum_{i=1}^l \\alpha_i = 1,\\\\\n",
    "0 \\leq \\alpha_i \\leq \\frac{1}{\\nu}C_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 (2 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, you have solved the dual and have optimal $(\\alpha^*_i)_{i=1}^l$ and\n",
    "$ h^* = \\sum_{i=1}^l \\alpha^*_i \\phi(x_i)$.\n",
    "\n",
    "Let $M = \\{i\\colon \\alpha^*_i \\in \\left(0, \\tfrac{C_i}{\\nu}\\right)\\}$ and suppose $M \\neq \\emptyset$.\n",
    "\n",
    "* Derive the expression for the optimal value of $R$ from this and the\n",
    "complementary sclackness conditions.\n",
    "\n",
    "* Write out the decision function for an arbitrary $x\\in \\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Consider condition $\\alpha_i(R + \\xi_i - \\|\\phi(x_i) - h \\|^2) = 0$. We can choose any solution of equations, for which $\\alpha \\neq 0$. For them from condition $\\alpha_i \\xi_i = 0$ we have $\\xi_i = 0$ . Thus:\n",
    "$$\n",
    "R = - \\xi_i + \\|\\phi(x_i) - h \\|^2 = \\langle \\phi(x_i);\\phi(x_i) \\rangle - 2\\langle \\phi(x_i);h \\rangle + \\langle h;h \\rangle = \\\\\n",
    "= K(x_i;x_i) - 2 \\sum_{j=1}^{l} \\alpha_j K(x_j;x_i) + \\sum_{i,j=1}^{l} \\alpha_i \\alpha_j K(x_j;x_i) \\text{,  for any $i$:  } \\alpha_i \\neq 0\n",
    "$$\n",
    "And decision function for $x$: \n",
    "$$\n",
    "\\hat{f(x)} = sign\\bigg[ R - K(x;x) - 2 \\sum_{j=1}^{l} \\alpha_j K(x_j;x) + \\sum_{i,j=1}^{l} \\alpha_i \\alpha_j K(x_j;x_i)\\bigg]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.1 (2 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that for some $x_i$ we have $\\|\\phi(x_i) - h\\|^2 < R$.\n",
    "We will call this point **inlier**.\n",
    "\n",
    "* What are the values of $\\alpha_i$ and $\\beta_i$ for such a point?\n",
    "* Show that $1-\\nu$ upper-bounds the sum of weights $C_i$ for the **inlier** points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's consider inlier $x_i$ $\\|\\phi(x_i) - h\\|^2 - R = \\delta_i < 0 $ and condition: $\\alpha_i(R + \\xi_i - \\|\\phi(x_i) - h \\|^2) = 0$. For $x_i$ we have:\n",
    "$$\n",
    "\\alpha_i(R + \\xi_i - \\|\\phi(x_i) - h \\|^2) = \\alpha_i(\\xi_i - \\delta_i) = \\alpha_i(\\xi_i + |\\delta_i|) = 0 \\rightarrow \\alpha_i = 0\n",
    "$$\n",
    "Thus from condition $\\alpha_i + \\beta_i = \\frac{1}{\\nu}C_i$ we have: \n",
    "$$\n",
    "\\beta = \\frac{1}{\\nu}C_i\n",
    "$$\n",
    "Now let's show that $1-\\nu$ upper-bounds the sum of weights $C_i$ for the **inlier** points. Denote set of inliners indexes as $S_{in}$, and others as $S_{ot}$:\n",
    "$$\n",
    "\\frac{1}{\\nu} = \\frac{1}{\\nu} \\sum_{i = 1}^{l}{C_i} =  \\sum_{i=1}^l \\alpha_i + \\sum_{i=1}^l \\beta_i = \\sum_{i \\in S_{in}} \\beta_i + \\sum_{i \\in S_{ot}} \\beta_i + 1 = \\sum_{i \\in S_{in}} \\frac{1}{\\nu}C_i + \\sum_{i \\in S_{ot}} \\beta_i + 1 \\rightarrow \\\\ \n",
    "\\rightarrow \\sum_{i \\in S_{in}} C_i = 1 - \\nu\\sum_{i \\in S_{ot}} \\beta_i -\\nu \\leq 1 -\\nu \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.2 (2 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that for some $x_i$ it holds that $\\|\\phi(x_i) - h \\|^2 > R$.\n",
    "Such points are called **outliers**.\n",
    "\n",
    "* What are the values of $\\alpha_i$ and $\\beta_i$ for these points?\n",
    "* Argue that the sum of weights $C_i$ for the **outliers** is upper bounded by $\\nu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For outlier $x_i$ we have $\\|\\phi(x_i) - h \\|^2 > R$. So, from conditions of inner problem one can see that $\\xi_i > 0$. Thus, from KKT condition $\\beta_i \\xi_i = 0 $ we have $\\beta_i = 0$. And,thus $\\alpha_i = \\frac{1}{\\nu}C_i$. Let's find sum of weights of outliers. Denote set of outliners indexes as $S_{out}$, and others as $S_{ot}$ : \n",
    "$$\n",
    "1 = \\sum_{i=1}^l \\alpha_i = \\sum_{i \\in S_{out}} \\alpha_i + \\sum_{i \\in S_{ot}} \\alpha_i = \\sum_{i \\in S_{out}} \\frac{1}{\\nu}C_i + \\sum_{i \\in S_{ot}} \\alpha_i \\rightarrow \\sum_{i \\in S_{out}} C_i = \\nu - \\nu \\sum_{i \\in S_{ot}} \\alpha_i < \\nu\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.3: Very small $\\nu$ (1 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that $\\nu < C_i$ for all $i=1,\\,\\ldots,\\,l$. Show that\n",
    "there are **no outliers** in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If $\\nu < C_i$ for all $i \\in {1,...,l}$ then for any outlier we will have $\\alpha_i = \\frac{1}{\\nu} C_i > C_i$. But we know that $\\alpha_i < C_i$. So, we have contradiction and thus if $\\nu \\leq C_i$ there is no outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.4 (1 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $C_i = \\tfrac1l$. Please, describe how $\\nu$ is related to the\n",
    "outliers in the datset, given the analysis above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Denote $n_{out}$ - number of outliers. Then,  $\\sum_{i \\in S_{out}} C_i = \\frac{n_{out}}{l} \\leq \\nu $. So, one can see  that $\\nu l$ is upper bound for number of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
