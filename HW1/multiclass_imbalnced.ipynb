{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "with zipfile.ZipFile('data/train.csv.zip') as data_archive:\n",
    "    dataset = pd.read_csv(data_archive.open('train.csv'), index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **id** -- anonimous idetificator;\n",
    "* **feat_1, ..., feat_93** -- anonymous feature;\n",
    "* **target** -- label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().T.sample(10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split target value and features. **LabelEncoder** transforms string into numbers from $0$ to $K-1$, where $K$ -- number of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(dataset.drop('target', axis=1), dtype=float)\n",
    "target = dataset['target']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder().fit(target)\n",
    "y = label_encoder.transform(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into train and validation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split = train_test_split(X, y, test_size=0.5,\n",
    "                         random_state=42, stratify=y)\n",
    "train_X, test_X, train_y, test_y = split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of such problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Multiclass** -- **single** label for every object\n",
    "    * Digit recognition, type of the product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Mulitlabel** -- possibly **several** labels for every element\n",
    "    * tags, list of objects in a picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will mostly work with **Mutliclass**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at, **LinearSVC**.\n",
    "This model works with **binary** classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/ovr.png\" title=\"one-vs-rest\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Fit**: for every $k \\in \\{1.. K\\}$ fit a classifier\n",
    " $h_k$, whcih can separate $k$ from other labels\n",
    "$y \\neq k$;\n",
    "\n",
    "```python\n",
    "def fit(X, y):\n",
    "    classifiers = []\n",
    "    for i in range(len(classes)):\n",
    "        y_i = np.where(y == classes[i], 1, 0)\n",
    "\n",
    "        classifiers.append(clone(classifier).fit(X, y_i))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict**: apply all $K$ classifiers for an element  $x$:\n",
    "    * select classifier with the biggets condifence\n",
    "\n",
    "```python\n",
    "def predict(X):\n",
    "    scores = []\n",
    "    for clf in classifiers:\n",
    "        scores.append(clf.predict_proba(X)[:, 1])\n",
    "\n",
    "    scores = np.stack(scores, axis=1)\n",
    "    return classes[np.argmax(scores, axis=1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "ovr_classifier = OneVsRestClassifier(clone(model), n_jobs=-1)\n",
    "ovr_classifier.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = ovr_classifier.predict(test_X)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(test_y, predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy %.3f%%\" % (100 * ovr_classifier.score(test_X, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/ovo.png\" title=\"one-vs-one\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Fit**: for every pair $i, j \\in \\{1.. K\\}$, $i \\prec j$,\n",
    "fit a clissifier $h_{ij}$, which separate $j$ from $i$;\n",
    "\n",
    "```python\n",
    "def fit(X, y):\n",
    "    classifiers, n_classes = {}, len(classes)\n",
    "    for i in range(n_classes):\n",
    "        for j in range(i+1, n_classes):\n",
    "            mask = (y == i) | (y == j)\n",
    "            \n",
    "            # j -- 1, i -- 0\n",
    "            y_ij = np.where(y[mask] == j, 1, 0)\n",
    "            classifiers[(i, j)] = clone(classifier).fit(X[mask], y_ij)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict**: apply all $\\frac12K (K-1)$ classifiers for an element $x$ and choose the most class with the largest amounts of votes\n",
    "\n",
    "```python\n",
    "def predict(X):\n",
    "    votes = np.zeros((n_samples, n_classes))\n",
    "    for i in range(n_classes):\n",
    "        for j in range(i+1, n_classes):\n",
    "            predicted = classifiers[(i, j)].predict(X)\n",
    "\n",
    "            votes[predicted == 0, i] += 1\n",
    "            votes[predicted == 1, j] += 1\n",
    "            \n",
    "    return classes[np.argmax(votes, axis=1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "ovo_classifier = OneVsOneClassifier(clone(model))\n",
    "\n",
    "ovo_classifier.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = ovo_classifier.predict(test_X)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(test_y, predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy %.3f%%\" % (100 * ovo_classifier.score(test_X, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Class | C_1 | C_2 | C_3 | C_4 | C_5 | C_6 | ... | C_L |\n",
    "|:-----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| 0     |  1  |  1  |  0  |  0  |  0  |  0  | ... |  0  |\n",
    "| 1     |  0  |  0  |  1  |  1  |  1  |  1  | ... |  0  |\n",
    "| 2     |  1  |  0  |  0  |  1  |  0  |  0  | ... |  1  |\n",
    "| ...   | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "| 8     |  1  |  1  |  0  |  1  |  0  |  1  | ... |  1  |\n",
    "| 9     |  0  |  1  |  1  |  1  |  0  |  0  | ... |  0  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Preprocessing**: for every label $\\{1..K\\}$ match a unique binary string of length $L$ -- class **code**;\n",
    "* **Fit**: binary classifiers $0-1$ for every position in the binary string;\n",
    "\n",
    "```python\n",
    "def fit(X, y):\n",
    "    classifiers, n_classes = [], len(classes)\n",
    "    code_book = ... \n",
    "                    \n",
    "\n",
    "    label_map = LabelEncoder().fit(y)\n",
    "    class_index = label_map.transform(y)\n",
    "    encoding = code_book.take(class_index, axis=0)\n",
    "\n",
    "    for i in range(n_code_size):\n",
    "        y_i = encoding[:, i]\n",
    "        classifiers.append(clone(classifier).fit(X, y_i))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to take a deeper look at one.vs.rest approach and do it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 0\n",
    "target_class_count = (train_y == target_class).sum()\n",
    "others_count = (train_y != target_class).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} target elements'.format(target_class_count))\n",
    "print('{} others'.format(others_count))\n",
    "print('{:.2f} imbalanced ratio'.format(others_count/target_class_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(y, t) = \\sum_{i=1}^N \\max(0, 1 - t_i \\cdot y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Loss = \\frac{n_1}{N}\\sum_{i, y_i =1}L(1, f(x_i)) + \\frac{n_{-1}}{N}\\sum_{i, y_i=-1}L(-1, f(x_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only one class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_X, train_y==target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy %.3f%%' % np.mean(predictions == (test_y == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_predictions = predictions[test_y == 1]\n",
    "negative_predictions = predictions[test_y != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Negative elements accuracy %.3f%%\" % (100 * (1 - negative_predictions.mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive elements accuracy %.3f%%\" % (100 * positive_predictions.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_values = model.decision_function(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hinge_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hinge loss %.3f\" % hinge_loss(test_y == 1, decision_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_decision_values = decision_values[test_y != 1]\n",
    "hinge_negative = np.mean(np.maximum(0, 1 + negative_decision_values))\n",
    "print(\"Negative elements hinge loss %.3f\" % hinge_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_decision_values = decision_values[test_y == 1]\n",
    "hinge_positive = np.mean(np.maximum(0, 1 - positive_decision_values))\n",
    "print(\"Positive elements hinge loss %.3f\" % hinge_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use code below to install imbalanced-learn package or do it manually with conda install -c conda-forge imbalanced-learn  \n",
    "!pip install --upgrade pip \n",
    "!pip install PyHamcrest\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balancer = RandomOverSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train_x, balanced_train_y = balancer.fit_sample(train_X, train_y == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "model.fit(balanced_train_x, balanced_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy %.3f%%' % (100 * np.mean(predictions == (test_y == 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_predictions = predictions[test_y == 1]\n",
    "negative_predictions = predictions[test_y != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Negative elements accuracy %.3f%%\" % (100 * (1 - negative_predictions.mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive elements accuracy %.3f%%\" % (100 * positive_predictions.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebalanced Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedLearner(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, classifier=LinearSVC, balancer=SMOTE):\n",
    "        self.classifier = classifier\n",
    "        self.balancer = balancer\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        train_X, train_y = self.balancer.fit_sample(X, y)\n",
    "        self.classifier.fit(train_X, train_y)\n",
    "\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        return self.classifier.decision_function(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalancer = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = BalancedLearner(clone(model), rebalancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_classifier = OneVsRestClassifier(clone(model_b))\n",
    "ovr_classifier.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y_balanced = ovr_classifier.predict(test_X)\n",
    "pd.DataFrame(confusion_matrix(test_y, predict_y_balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core import display as ICD\n",
    "\n",
    "print('Balanced')\n",
    "ICD.display(pd.DataFrame(confusion_matrix(test_y, predict_y_balanced)))\n",
    "print('Original')\n",
    "ICD.display(pd.DataFrame(confusion_matrix(test_y, predict_y)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
