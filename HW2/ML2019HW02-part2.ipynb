{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Part 2 (Practice)\n",
    "To solve this task, you will write a lot of code to try several machine learning methods for classification and regression.\n",
    "* You are **HIGHLY RECOMMENDED** to read relevant documentation, e.g. for [python](https://docs.python.org/3/), [numpy](https://docs.scipy.org/doc/numpy/reference/), [matlpotlib](https://matplotlib.org/) and [sklearn](https://scikit-learn.org/stable/). Also remember that seminars, lecture slides, [Google](http://google.com) and [StackOverflow](https://stackoverflow.com/) are your close friends during this course (and, probably, whole life?).\n",
    "\n",
    "* If you want an easy life, you have to use **BUILT-IN METHODS** of `sklearn` library instead of writing tons of our yown code. There exists a class/method for almost everything you can imagine (related to this homework).\n",
    "\n",
    "* To do this part of homework, you have to write **CODE** directly inside specified places inside notebook **CELLS**.\n",
    "\n",
    "* In some problems you may be asked to provide short discussion of the results. In this cases you have to create **MARKDOWN** cell with your comments right after the your code cell.\n",
    "\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**. So make sure that you did everything required in the task\n",
    "\n",
    "* Your **SOLUTION** notebook **MUST BE REPRODUCIBLE**, i.e. if the reviewer decides to execute `Kernel` -> `Restart Kernel and Run All Cells`, after all the computation he will obtain exactly the same solution (with all the corresponding plots) as in your uploaded notebook. For this purpose, we suggest to fix random `seed` or (better) define `random_state=` inside every algorithm that uses some pseudorandomness.\n",
    "\n",
    "* Your code must be clear to the reviewer. For this purpose, try to include neccessary comments inside the code. But remember: **GOOD CODE MUST BE SELF-EXPLANATORY** without any additional comments.\n",
    "\n",
    "Before the start, read several additional recommendations.\n",
    "* Probably you lauch `jupyter notebook` or `ipython notebook` from linux console. Try `jupyter lab` instead - it is a more convenient environment to work with notebooks.\n",
    "* Probably the PC on which you are going to evaluate models has limited CPU/RAM Memory. In this case, we recommend to monitor the CPU and Memory Usage. To do this, you can execute `htop` (for CPU/RAM) or `free -s 0.2` (for RAM) in terminal.\n",
    "* Probably tou have multiple Cores (CPU) on your PC. Many `sklearn` algorithms support multithreading (Ensemble Methods, Cross-Validation, etc.). Check if the particular algorithm has `n_jobs` parameters and set it to `-1` to use all the cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, write your implementation within the designated blocks:\n",
    "```python\n",
    "...\n",
    "### BEGIN Solution\n",
    "\n",
    "# >>> your solution here <<<\n",
    "\n",
    "### END Solution\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_fs = pd.read_csv(r'data/data_fs.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first 10 rows of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has many NaN's and also a lot of categorical features. So at first, you should preprocess the data. We can deal with categorical features by using one-hot encoding. To do that we can use [`pandas.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill nan with 0\n",
    "data_fs = data_fs.fillna(0)\n",
    "\n",
    "# our goal is to predict the \"price_doc\" feature.\n",
    "y = data_fs[[\"price_doc\"]]\n",
    "X = data_fs.drop(\"price_doc\", axis=1)\n",
    "X = X.drop(\"timestamp\", axis=1)\n",
    "\n",
    "# one-hot encoding\n",
    "X = pd.get_dummies(X, sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split our dataset into train 70 % and test 30% by using sklearn. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Look at first 10 rows what you get.\n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's see how much data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train size =\", X_train.shape)\n",
    "print(\"Test size =\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are too many features in this dataset and not all of them are equally important for our problem. Besides, using the whole dataset as-is to train a linear model will, for sure, lead to overfitting. Instead of painful and time consuming manual selection of the most relevant data, we will use the methods of automatic feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But at first, we almost forgot to take a look at our targets. Let's plot `y_train` histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big variance in it and it's far from being a normal distribution. In the real-world problems it happens all the time: the data can be far from perfect. We can use some tricks to make it more like what we want.\n",
    "In this particular case we can predict $\\log y$ instead of $y$. This transformation is invertible, so we will be able to get our $y$ back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)\n",
    "y_train_log.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it looks more like the data we want to deal with.\n",
    "\n",
    "The preprocessing is finally over, so now we are ready for the actual task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center><span style=\"color:red;\">**IMPORTANT NOTICE**</span></center></h3>\n",
    "\n",
    "If you have difficulties with solving the below problems take a look at seminar $7$ on feature and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (1 pt.): Random forest feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use random forest to find the imortance of features. Plot the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "### BEGIN Solution\n",
    "\n",
    "\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the 20 most important features and their **values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (1 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On these 20 features train each of the following models\n",
    "* **Linear Regression**\n",
    "* **Ridge regression**\n",
    "* **Random forest**\n",
    "* **DecisionTree**\n",
    "\n",
    "and test its performance using the **Root Mean Squared Logarithmic Error** (RMSLE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to do it for the next tasks too, so we recommend you to implement\n",
    "a dedicated function for comparisons, which\n",
    "1. on input the function takes a training dataset `(X_train, y_train)` and a test sample `(X_test, y_test)`\n",
    "2. it trains **all of the listed models** on the `(X_train, y_train)` sample\n",
    "3. it computes and returns a table the RMSLE score of each fitted model on the test dataset`(X_test, y_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "def comparator(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "        X_train: ndarray - training inputs\n",
    "        y_train: ndarray - training targets\n",
    "        X_test: ndarray - test inputs\n",
    "        y_test: ndarray - test targets\n",
    "        \n",
    "    Returns\n",
    "    =======\n",
    "        pd.DataFrame - table of RMSLE scores of each model on test and train datasets\n",
    "    \"\"\"\n",
    "    methods = {\n",
    "        \"Linear Regression\": sklearn.linear_model.LinearRegression(), \n",
    "        \"Lasso\": linear_model.Lasso(), \n",
    "        \"Ridge\": linear_model.Ridge(),\n",
    "        \"Dtree\": sklearn.tree.DecisionTreeRegressor(),\n",
    "        \"RFR\": sklearn.ensemble.RandomForestRegressor(n_estimators =100)\n",
    "    }\n",
    "\n",
    "### BEGIN Solution\n",
    "\n",
    "\n",
    "\n",
    "### END Solution\n",
    "    return pd.DataFrame({\n",
    "        \"Methods\": list(methods.keys()),\n",
    "        \"Train loss\": error_train,\n",
    "        \"Test loss\": error_test\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-backward methods\n",
    "\n",
    "The idea is to add or remove features and look how it influences the value of the loss function or some other criteria.\n",
    "\n",
    "Decision about adding or deleting a feature may be made based on:\n",
    "\n",
    "- AIC\n",
    "- BIC\n",
    "- validation error\n",
    "- Mallows $C_p$\n",
    "- sklearn's `estimator.score()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (2 pt.): Implement forward method with early stopping\n",
    "\n",
    "Implement the following greedy feature selection algorithm:\n",
    "\n",
    "```python\n",
    "# Initialize with an empty list of features.\n",
    "list_of_best_features = []\n",
    "\n",
    "while round < n_rounds:\n",
    "    round = round + 1\n",
    "    \n",
    "    if no_more_features:\n",
    "        # end loop\n",
    "\n",
    "    # Iterate over currently *unsued* features and use $k$-fold \n",
    "    # . `cross_val_score` to measure model \"quality\".\n",
    "    compute_quality_with_each_new_unused_feature(...)\n",
    "\n",
    "    # **Add** the feature that gives the highest \"quality\" of the model.\n",
    "    pick_and_add_the_best_feature(...)\n",
    "\n",
    "    if model_quality_has_increased_since_last_round:\n",
    "        round = 0\n",
    "\n",
    "return list_of_best_features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:red\">ATTN</span>**\n",
    "Use $k=3$ for the $k$-fold cv, because higher values could take a **lo-o-o-o-o-o-o-o-ong** time.\n",
    "\n",
    "Please bear in mind that **the lower** RMSLE (`mean_squared_log_error`) is, **the higher the model \"quality\" is**.\n",
    "\n",
    "Please look up `cross_val_score(...)` peculiarities in [scikit's manual](https://scikit-learn.org/stable/documentation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below implement a function that would iterate over a list of features and use $k$-fold `cross_val_score` to measure model \"quality\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def selection_step(model, X, y, used_features=(), cv=3):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "        X: ndarray - training inputs\n",
    "        y: ndarray - training targets\n",
    "        used_features: - list of features\n",
    "        cv: int - number of folds\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "        scores - dictionary of scores\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    ### BEGIN Solution\n",
    "\n",
    "    ### END Solution\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_steps(X, y, n_rounds, method):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "        X: ndarray - training inputs\n",
    "        y: ndarray - training targets\n",
    "        n_rounds: int - early stop when score doesn't increase n_rounds\n",
    "        method: sklearn model\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "        feat_best_list - list of features\n",
    "    \"\"\"\n",
    "    \n",
    "    feat_best_list = []\n",
    "\n",
    "    ### BEGIN Solution\n",
    "\n",
    "    ### END Solution\n",
    "    \n",
    "    return feat_best_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function implemented above and use DecisionTreeRegressor to get the best features according to this algorithm and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Linear Regression, Ridge regression, Random forest and DecisionTree to get the RMSLE score using these features. Remember the function you wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting: gradient boosting, adaboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you are asked to implement a boosting algorithm, and compare speed of\n",
    "different popular boosting libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (2 pt.): Boosting Classification on a toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a toy dataset for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=300, shuffle=True, noise=0.05, random_state=1011)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is:\n",
    "1. Implement gradient boosting algorithms with **logistic loss**\n",
    "and labels $y\\in \\{-1, +1\\}$;\n",
    "2. **Plot the decision boundary** on a $2$-d grid; \n",
    "3. Estimate the accuracy **score** on the test dataset, as well\n",
    "as other classification metrics, that you can think of;\n",
    "    \n",
    "For basic implementation please refer to seminars $8-9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 (1 pt.): Measuring the Speed and Performance\n",
    "\n",
    "Please make sure to install the following powerful packages for boosting:\n",
    "* [xgboost](https://anaconda.org/conda-forge/xgboost)\n",
    "* [lightgbm](https://anaconda.org/conda-forge/lightgbm)\n",
    "* [catboost](https://tech.yandex.com/catboost/doc/dg/concepts/python-installation-docpage/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you are asked to compare the **training time** of the **GBDT**, the\n",
    "Gradient Boosted Decision Trees, as implemeted by different popular ML libraries.\n",
    "The dataset you shall use is the [UCI Breast Cancer dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).\n",
    "You should study the parameters of each library and establish the **correspondence**\n",
    "between them.\n",
    "\n",
    "The plan is as follows:\n",
    "1. Take the **default** parameter settings, measure the training time, and plot\n",
    "the ROC curves;\n",
    "2. Use grid search with the $3$-fold cross valiadation to choose the best model.\n",
    "Then measure the training time as a function of (separately) **tree depth** and **the\n",
    "number of estimators in the ensemble**, finally **plot the ROC** curves of the best\n",
    "models.\n",
    "\n",
    "You need to make sure that you are comparing **comparable** classifiers, i.e. with\n",
    "**the same tree and ensemble hyperparameters**.\n",
    "\n",
    "<span style=\"color:green\">**NOTE**</span> You need figure out how to make parameter settings\n",
    "compatible. One possible way to understand the correspondence is to study the docs. You may\n",
    "choose the default parameters from any library.\n",
    "\n",
    "Please plot **three** ROC curves, one per library, on the same **one plot**\n",
    "with a *comprehensible [legend](https://matplotlib.org/users/legend_guide.html)*.\n",
    "\n",
    "A useful command for timing is IPython's [**timeit** cell magic](http://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,\n",
    "                                                    random_state=0x0BADBEEF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 (1 pt.): Activation functions\n",
    "Plot the following [activation functions](https://pytorch.org/docs/master/nn.html#non-linear-activation-functions) using their PyTorch realizations and their derivatives using autograd functionality:\n",
    "* ReLU, ELU ($\\alpha = 1$), Softplus ($\\beta = 1$);\n",
    "* Sign, Sigmoid, Softsign, Tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "x = torch.arange(-2, 2, .01, requires_grad=True)\n",
    "x.sum().backward() # to create x.grad\n",
    "\n",
    "f, axes = plt.subplots(2, 2, sharex=True, figsize=(15, 7))\n",
    "axes[0, 0].set_title('Values')\n",
    "axes[0, 1].set_title('Derivatives')\n",
    "\n",
    "for i, function_set in (0, (('ReLU', F.relu), ('ELU', F.elu), ('Softplus', F.softplus))), \\\n",
    "                       (1, (('Sign', torch.sign), ('Sigmoid', torch.sigmoid), ('Softsign', F.softsign), ('Tanh', torch.tanh))):\n",
    "    for function_name, activation in function_set:\n",
    "        ### BEGIN Solution\n",
    "        # ...\n",
    "        # axes[i, 0].plot('xs', 'funcion values', label=function_name)\n",
    "        # axes[i, 1].plot('xs', 'derivative values', label=function_name)\n",
    "        ### END Solution\n",
    "\n",
    "    axes[i, 0].legend()\n",
    "    axes[i, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions. Which of these functions may be, and which -- definitely are a poor choise as an activation function in a neural network? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 (3 pt.): Backpropagation\n",
    "At the seminar 10 on neural networks, we built an MLP with one hidden layer using our numpy implementations of linear layer and logistic and softmax activation functions. Your task is to\n",
    "1. implement backpropagation for these modules,\n",
    "2. train our numpy realization of MLP to classify the toy MNIST from `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits, targets = load_digits(return_X_y=True)\n",
    "digits = digits.astype(np.float32) / 255\n",
    "\n",
    "digits_train, digits_test, targets_train, targets_test = train_test_split(digits, targets, random_state=0)\n",
    "\n",
    "train_size = digits_train.shape[0]\n",
    "\n",
    "input_size = 8*8\n",
    "classes_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the MLP with backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.thetas = np.random.randn(input_size, output_size)\n",
    "        self.thetas_grads = np.empty(self.thetas)\n",
    "        self.bias = np.random.randn(output_size)\n",
    "        self.bias_grads = np.empty(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        output = np.matmul(x, self.thetas) + self.bias\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def backward(self, x, output_grad):\n",
    "        ### BEGIN Solution\n",
    "        # ... calculate grads\n",
    "        # self.thetas_grads += \n",
    "        # self.bias_grads += \n",
    "        ### END Solution\n",
    "        return input_grad\n",
    "\n",
    "\n",
    "class LogisticActivation:\n",
    "    def forward(self, x):\n",
    "        output = 1/(1 + np.exp(-x))\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward(self, x, output_grad):\n",
    "        ### BEGIN Solution\n",
    "        # ... calculate grads\n",
    "        ### END Solution\n",
    "        return input_grad\n",
    "    \n",
    "\n",
    "class SoftMaxActivation:\n",
    "    def forward(self, x):\n",
    "        output = np.exp(x) / np.exp(x).sum(axis=-1, keepdims=True)\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def backward(self, x, output_grad):\n",
    "        ### BEGIN Solution\n",
    "        # ... calculate grads\n",
    "        ### END Solution\n",
    "        return input_grad\n",
    "    \n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        self.linear1 = Linear(input_size, hidden_layer_size)\n",
    "        self.activation1 = LogisticActivation()\n",
    "        self.linear2 = Linear(hidden_layer_size, output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return linear2.forward(activation1.forward(self.linear1.forward(x)))\n",
    "\n",
    "\n",
    "    def backward(self, x, output_grad):\n",
    "        ### BEGIN Solution\n",
    "        # ... calculate and update grads\n",
    "        ### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "# Please, implement here everything else you need, like the loss function.\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "mlp = MLP(input_size=input_size, hidden_layer_size=100, output_size=classes_n)\n",
    "\n",
    "epochs_n = 200\n",
    "learning_curve = [0] * epochs_n\n",
    "test_curve = [0] * epochs_n\n",
    "\n",
    "x_train = digits_train\n",
    "x_test = digits_test\n",
    "y_train = targets_train\n",
    "y_test = targets_test\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "for epoch in range(epochs_n):\n",
    "    if epoch % 10 == 0:\n",
    "        print('Starting epoch {}'.format(epoch), end=' ')\n",
    "    for sample_i in range(train_size):\n",
    "        x = x_train[sample_i]\n",
    "        target = y_train[sample_i]\n",
    "\n",
    "        ### BEGIN Solution\n",
    "        # ... zero the gradients\n",
    "        # prediction = mlp.forward(x)\n",
    "        # loss = # use cross entropy loss\n",
    "        # learning_curve[epoch] += loss\n",
    "        # ... perform backward pass\n",
    "        # ... update the weights simply with weight -= grad * learning_rate\n",
    "    \n",
    "    # learning_curve[epoch] /= train_size\n",
    "    # prediction = mlp.forward(x_test)\n",
    "    # loss = # use cross entropy loss\n",
    "    # test_curve[epoch] = loss\n",
    "    ### END Solution\n",
    "\n",
    "\n",
    "plt.plot(learning_curve)\n",
    "plt.plot(test_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, predictions = np.max(mlp.forward(digits), -1)\n",
    "pd.DataFrame(confusion_matrix(targets, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 (3 pt.): Modelling real-life DL\n",
    "In this task you will train your own CNN for dogs vs cats classification task. The goal of this task is not to get the highest accuracy possible (try getting the highest accuracy possible though) but to model the real-life process of training a deep neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center><span style=\"color:red;\">**IMPORTANT NOTICE**</span></center></h3>\n",
    "Training neural networks is a time consuming task and it can take days or even weeks. Try not to leave this task to the last day. It is not necessary for you to use GPU for this task, but using it may drastically reduce the time required for you to complete this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a good amount of datasets in [torchvision](https://pytorch.org/docs/stable/torchvision/datasets.html), but in practice, chances are that you wouldn't find the dataset for your particular problem, so you should be capable of writing `DataLoader` for your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import PIL.Image as Image\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you are using the right device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_csv(r'data/cats_dogs/train.csv')\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('data/' + dt['path'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change class name \n",
    "class Your_class(Dataset):\n",
    "    \"\"\" Some documantation\"\"\"\n",
    "\n",
    "    def __init__(self,csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        \"\"\"\n",
    "        ### BEGIN Solution\n",
    "        # ... here you can load and initialize what you will need next\n",
    "        ### END Solution\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ### BEGIN Solution\n",
    "        # ... don't forget to augment your data for training, using the `transform` parameter of the constructor\n",
    "        ### END Solution\n",
    "        return img, torch.tensor(y)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        ### BEGIN Solution\n",
    "        # ... \n",
    "        ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the augmentation tranform and instantiate training and validation subsets of your `Dataset` and the correpsonding [`DataLoaders`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    ### BEGIN Solution\n",
    "    # ...\n",
    "    ### END Solution\n",
    "    ])\n",
    "\n",
    "### BEGIN Solution\n",
    "# dataset_train = \n",
    "# dataset_val = \n",
    "# train_loader = \n",
    "# val_loader = \n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that dataloader works as expected by observing one sample from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for X,y in train_loader:\n",
    "    print(X[0])\n",
    "    print(y[0])\n",
    "    plt.imshow(np.array(X[0,0,:,:]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your model below. You can use any layers that you want, but in general the structure of your model should be\n",
    "1. convolutional feature extractor, followed by\n",
    "2. fully-connected classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model_name(nn.Module):\n",
    "    def __init__(self, inp_ch=1, outp_ch=2):\n",
    "        super().__init__()\n",
    "        ### BEGIN Solution\n",
    "        # ...\n",
    "        ### END Solution\n",
    "       \n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        ### BEGIN Solution\n",
    "        # ...\n",
    "        ### END Solution\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send your model to GPU, if you have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model_name().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your loss function below, or use the predefined loss, suitable for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "# criterion = #\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try two different [optimizers](http://ruder.io/optimizing-gradient-descent/index.html) and choose one. For the optimizer of your choice, try two different sets of parameters (e.g learning rate). Explain both of your choices and back them with the learning performance of the network (see the rest of the task).\n",
    "\n",
    "In this parts of the task you may try more than two options, but, please, leave in your solution only the results for two different optimizers and two different sets of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "#optimizer = \n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may finally train you model. Don't forget to:\n",
    "1. monitor its training and validation performance *during training*, i.e plot the loss functions and prediction accuracy for train and validation sets, to make sure that your model doesn't learn complete nonsense; **do not** include tons of learning curves in your homework solution; (in real-life, you may find [`tensorboardX`](https://github.com/lanpa/tensorboardX) extremely useful for this task);\n",
    "2. visualize its training and validation performance *after training*, to demonstrate that you have accomplished the task;\n",
    "3. save the state of your model during the training, to use the best one at the end; you may find useful this [tutorial on saving and loading models](https://pytorch.org/tutorials/beginner/saving_loading_models.html);\n",
    "4. send the input and target data to the same device as your model.\n",
    "\n",
    "Your model should be able to show *at least 75% validation accuracy*.\n",
    "\n",
    "You may also find useful the following parts of documentation: [`Module.train`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.train), [`Module.eval`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval), [`Module.state_dict`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.state_dict), [`Module.load_state_dict`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.load_state_dict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9 (1 pt.): Bad activation function\n",
    "Using your conclusions from the <span style=\"color:red;\">Task 6</span>, choose the worst activation function and replace all activations in your model from the previous <span style=\"color:red;\">Task 8</span> with this one. Demonstrate the training and validation performance of this version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
