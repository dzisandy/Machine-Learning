{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Part 2 (Practice)\n",
    "To solve this task, you will write a lot of code to try several machine learning methods for classification and regression.\n",
    "* You are **HIGHLY RECOMMENDED** to read relevant documentation, e.g. for [python](https://docs.python.org/3/), [numpy](https://docs.scipy.org/doc/numpy/reference/), [matlpotlib](https://matplotlib.org/) and [sklearn](https://scikit-learn.org/stable/). Also remember that seminars, lecture slides, [Google](http://google.com) and [StackOverflow](https://stackoverflow.com/) are your close friends during this course (and, probably, whole life?).\n",
    "\n",
    "* If you want an easy life, you have to use **BUILT-IN METHODS** of `sklearn` library instead of writing tons of our yown code. There exists a class/method for almost everything you can imagine (related to this homework).\n",
    "\n",
    "* To do this part of homework, you have to write **CODE** directly inside specified places inside notebook **CELLS**.\n",
    "\n",
    "* In some problems you may be asked to provide short discussion of the results. In this cases you have to create **MARKDOWN** cell with your comments right after the your code cell.\n",
    "\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**. So make sure that you did everything required in the task\n",
    "\n",
    "* Your **SOLUTION** notebook **MUST BE REPRODUCIBLE**, i.e. if the reviewer decides to execute `Kernel` -> `Restart Kernel and Run All Cells`, after all the computation he will obtain exactly the same solution (with all the corresponding plots) as in your uploaded notebook. For this purpose, we suggest to fix random `seed` or (better) define `random_state=` inside every algorithm that uses some pseudorandomness.\n",
    "\n",
    "* Your code must be clear to the reviewer. For this purpose, try to include neccessary comments inside the code. But remember: **GOOD CODE MUST BE SELF-EXPLANATORY** without any additional comments.\n",
    "\n",
    "Before the start, read several additional recommendations.\n",
    "* Probably you lauch `jupyter notebook` or `ipython notebook` from linux console. Try `jupyter lab` instead - it is a more convenient environment to work with notebooks.\n",
    "* Probably the PC on which you are going to evaluate models has limited CPU/RAM Memory. In this case, we recommend to monitor the CPU and Memory Usage. To do this, you can execute `htop` (for CPU/RAM) or `free -s 0.2` (for RAM) in terminal.\n",
    "* Probably tou have multiple Cores (CPU) on your PC. Many `sklearn` algorithms support multithreading (Ensemble Methods, Cross-Validation, etc.). Check if the particular algorithm has `n_jobs` parameters and set it to `-1` to use all the cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, write your implementation within the designated blocks:\n",
    "```python\n",
    "...\n",
    "### BEGIN Solution\n",
    "\n",
    "# >>> your solution here <<<\n",
    "\n",
    "### END Solution\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fs = pd.read_csv(r'data/data_fs.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first 10 rows of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>full_sq</th>\n",
       "      <th>life_sq</th>\n",
       "      <th>floor</th>\n",
       "      <th>max_floor</th>\n",
       "      <th>material</th>\n",
       "      <th>build_year</th>\n",
       "      <th>num_room</th>\n",
       "      <th>kitch_sq</th>\n",
       "      <th>state</th>\n",
       "      <th>...</th>\n",
       "      <th>provision_retail_space_modern_sqm</th>\n",
       "      <th>turnover_catering_per_cap</th>\n",
       "      <th>theaters_viewers_per_1000_cap</th>\n",
       "      <th>seats_theather_rfmin_per_100000_cap</th>\n",
       "      <th>museum_visitis_per_100_cap</th>\n",
       "      <th>bandwidth_sports</th>\n",
       "      <th>population_reg_sports_share</th>\n",
       "      <th>students_reg_sports_share</th>\n",
       "      <th>apartment_build</th>\n",
       "      <th>apartment_fund_sqm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-08-20</td>\n",
       "      <td>43</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>271.0</td>\n",
       "      <td>6943.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.45356</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>269768.0</td>\n",
       "      <td>22.37</td>\n",
       "      <td>64.12</td>\n",
       "      <td>23587.0</td>\n",
       "      <td>230310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-08-23</td>\n",
       "      <td>34</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>271.0</td>\n",
       "      <td>6943.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.45356</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>269768.0</td>\n",
       "      <td>22.37</td>\n",
       "      <td>64.12</td>\n",
       "      <td>23587.0</td>\n",
       "      <td>230310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-08-27</td>\n",
       "      <td>43</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>271.0</td>\n",
       "      <td>6943.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.45356</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>269768.0</td>\n",
       "      <td>22.37</td>\n",
       "      <td>64.12</td>\n",
       "      <td>23587.0</td>\n",
       "      <td>230310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>89</td>\n",
       "      <td>50.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>271.0</td>\n",
       "      <td>6943.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.45356</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>269768.0</td>\n",
       "      <td>22.37</td>\n",
       "      <td>64.12</td>\n",
       "      <td>23587.0</td>\n",
       "      <td>230310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-09-05</td>\n",
       "      <td>77</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>271.0</td>\n",
       "      <td>6943.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.45356</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>269768.0</td>\n",
       "      <td>22.37</td>\n",
       "      <td>64.12</td>\n",
       "      <td>23587.0</td>\n",
       "      <td>230310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2011-09-06</td>\n",
       "      <td>67</td>\n",
       "      <td>46.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>271.0</td>\n",
       "      <td>6943.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.45356</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>269768.0</td>\n",
       "      <td>22.37</td>\n",
       "      <td>64.12</td>\n",
       "      <td>23587.0</td>\n",
       "      <td>230310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2011-09-08</td>\n",
       "      <td>25</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>271.0</td>\n",
       "      <td>6943.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.45356</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>269768.0</td>\n",
       "      <td>22.37</td>\n",
       "      <td>64.12</td>\n",
       "      <td>23587.0</td>\n",
       "      <td>230310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2011-09-09</td>\n",
       "      <td>44</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>271.0</td>\n",
       "      <td>6943.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.45356</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>269768.0</td>\n",
       "      <td>22.37</td>\n",
       "      <td>64.12</td>\n",
       "      <td>23587.0</td>\n",
       "      <td>230310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2011-09-10</td>\n",
       "      <td>42</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>271.0</td>\n",
       "      <td>6943.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.45356</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>269768.0</td>\n",
       "      <td>22.37</td>\n",
       "      <td>64.12</td>\n",
       "      <td>23587.0</td>\n",
       "      <td>230310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2011-09-13</td>\n",
       "      <td>36</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>271.0</td>\n",
       "      <td>6943.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.45356</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>269768.0</td>\n",
       "      <td>22.37</td>\n",
       "      <td>64.12</td>\n",
       "      <td>23587.0</td>\n",
       "      <td>230310.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 390 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  full_sq  life_sq  floor  max_floor  material  build_year  \\\n",
       "0  2011-08-20       43     27.0    4.0        NaN       NaN         NaN   \n",
       "1  2011-08-23       34     19.0    3.0        NaN       NaN         NaN   \n",
       "2  2011-08-27       43     29.0    2.0        NaN       NaN         NaN   \n",
       "3  2011-09-01       89     50.0    9.0        NaN       NaN         NaN   \n",
       "4  2011-09-05       77     77.0    4.0        NaN       NaN         NaN   \n",
       "5  2011-09-06       67     46.0   14.0        NaN       NaN         NaN   \n",
       "6  2011-09-08       25     14.0   10.0        NaN       NaN         NaN   \n",
       "7  2011-09-09       44     44.0    5.0        NaN       NaN         NaN   \n",
       "8  2011-09-10       42     27.0    5.0        NaN       NaN         NaN   \n",
       "9  2011-09-13       36     21.0    9.0        NaN       NaN         NaN   \n",
       "\n",
       "   num_room  kitch_sq  state         ...          \\\n",
       "0       NaN       NaN    NaN         ...           \n",
       "1       NaN       NaN    NaN         ...           \n",
       "2       NaN       NaN    NaN         ...           \n",
       "3       NaN       NaN    NaN         ...           \n",
       "4       NaN       NaN    NaN         ...           \n",
       "5       NaN       NaN    NaN         ...           \n",
       "6       NaN       NaN    NaN         ...           \n",
       "7       NaN       NaN    NaN         ...           \n",
       "8       NaN       NaN    NaN         ...           \n",
       "9       NaN       NaN    NaN         ...           \n",
       "\n",
       "  provision_retail_space_modern_sqm turnover_catering_per_cap  \\\n",
       "0                             271.0                    6943.0   \n",
       "1                             271.0                    6943.0   \n",
       "2                             271.0                    6943.0   \n",
       "3                             271.0                    6943.0   \n",
       "4                             271.0                    6943.0   \n",
       "5                             271.0                    6943.0   \n",
       "6                             271.0                    6943.0   \n",
       "7                             271.0                    6943.0   \n",
       "8                             271.0                    6943.0   \n",
       "9                             271.0                    6943.0   \n",
       "\n",
       "   theaters_viewers_per_1000_cap  seats_theather_rfmin_per_100000_cap  \\\n",
       "0                          565.0                              0.45356   \n",
       "1                          565.0                              0.45356   \n",
       "2                          565.0                              0.45356   \n",
       "3                          565.0                              0.45356   \n",
       "4                          565.0                              0.45356   \n",
       "5                          565.0                              0.45356   \n",
       "6                          565.0                              0.45356   \n",
       "7                          565.0                              0.45356   \n",
       "8                          565.0                              0.45356   \n",
       "9                          565.0                              0.45356   \n",
       "\n",
       "   museum_visitis_per_100_cap  bandwidth_sports  population_reg_sports_share  \\\n",
       "0                      1240.0          269768.0                        22.37   \n",
       "1                      1240.0          269768.0                        22.37   \n",
       "2                      1240.0          269768.0                        22.37   \n",
       "3                      1240.0          269768.0                        22.37   \n",
       "4                      1240.0          269768.0                        22.37   \n",
       "5                      1240.0          269768.0                        22.37   \n",
       "6                      1240.0          269768.0                        22.37   \n",
       "7                      1240.0          269768.0                        22.37   \n",
       "8                      1240.0          269768.0                        22.37   \n",
       "9                      1240.0          269768.0                        22.37   \n",
       "\n",
       "   students_reg_sports_share  apartment_build  apartment_fund_sqm  \n",
       "0                      64.12          23587.0            230310.0  \n",
       "1                      64.12          23587.0            230310.0  \n",
       "2                      64.12          23587.0            230310.0  \n",
       "3                      64.12          23587.0            230310.0  \n",
       "4                      64.12          23587.0            230310.0  \n",
       "5                      64.12          23587.0            230310.0  \n",
       "6                      64.12          23587.0            230310.0  \n",
       "7                      64.12          23587.0            230310.0  \n",
       "8                      64.12          23587.0            230310.0  \n",
       "9                      64.12          23587.0            230310.0  \n",
       "\n",
       "[10 rows x 390 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has many NaN's and also a lot of categorical features. So at first, you should preprocess the data. We can deal with categorical features by using one-hot encoding. To do that we can use [`pandas.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan with 0\n",
    "data_fs = data_fs.fillna(0)\n",
    "\n",
    "# our goal is to predict the \"price_doc\" feature.\n",
    "y = data_fs[[\"price_doc\"]]\n",
    "X = data_fs.drop(\"price_doc\", axis=1)\n",
    "X = X.drop(\"timestamp\", axis=1)\n",
    "\n",
    "# one-hot encoding\n",
    "X = pd.get_dummies(X, sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_sq</th>\n",
       "      <th>life_sq</th>\n",
       "      <th>floor</th>\n",
       "      <th>max_floor</th>\n",
       "      <th>material</th>\n",
       "      <th>build_year</th>\n",
       "      <th>num_room</th>\n",
       "      <th>kitch_sq</th>\n",
       "      <th>state</th>\n",
       "      <th>area_m</th>\n",
       "      <th>...</th>\n",
       "      <th>child_on_acc_pre_school_3,013</th>\n",
       "      <th>child_on_acc_pre_school_7,311</th>\n",
       "      <th>modern_education_share_0</th>\n",
       "      <th>modern_education_share_90,92</th>\n",
       "      <th>modern_education_share_93,08</th>\n",
       "      <th>modern_education_share_95,4918</th>\n",
       "      <th>old_education_build_share_0</th>\n",
       "      <th>old_education_build_share_23,14</th>\n",
       "      <th>old_education_build_share_25,47</th>\n",
       "      <th>old_education_build_share_8,2517</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14065</th>\n",
       "      <td>46</td>\n",
       "      <td>44.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.139168e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12978</th>\n",
       "      <td>77</td>\n",
       "      <td>48.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.631523e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18695</th>\n",
       "      <td>39</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.293465e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26411</th>\n",
       "      <td>52</td>\n",
       "      <td>52.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.553630e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>30</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.641243e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29787</th>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.441296e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18411</th>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.139168e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11541</th>\n",
       "      <td>31</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.662813e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20741</th>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.677245e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13103</th>\n",
       "      <td>58</td>\n",
       "      <td>42.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1974.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.389199e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 560 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       full_sq  life_sq  floor  max_floor  material  build_year  num_room  \\\n",
       "14065       46     44.0    7.0       25.0       1.0      2015.0       1.0   \n",
       "12978       77     48.0   17.0       17.0       4.0      2009.0       3.0   \n",
       "18695       39     18.0    7.0       17.0       1.0         0.0       1.0   \n",
       "26411       52     52.0    9.0       17.0       1.0         0.0       2.0   \n",
       "1419        30     18.0    1.0        0.0       0.0         0.0       0.0   \n",
       "29787       99      0.0   12.0        0.0       1.0      2015.0       4.0   \n",
       "18411       40      0.0   17.0       17.0       1.0         0.0       1.0   \n",
       "11541       31     17.0    1.0        9.0       2.0      1964.0       1.0   \n",
       "20741       55      0.0    6.0        0.0       1.0         0.0       2.0   \n",
       "13103       58     42.0    7.0        9.0       1.0      1974.0       3.0   \n",
       "\n",
       "       kitch_sq  state        area_m                ...                 \\\n",
       "14065       1.0    1.0  1.139168e+07                ...                  \n",
       "12978       9.0    3.0  1.631523e+07                ...                  \n",
       "18695       9.0    0.0  5.293465e+06                ...                  \n",
       "26411       1.0    1.0  2.553630e+07                ...                  \n",
       "1419        0.0    0.0  2.641243e+06                ...                  \n",
       "29787       1.0    1.0  4.441296e+06                ...                  \n",
       "18411       1.0    1.0  1.139168e+07                ...                  \n",
       "11541       6.0    2.0  4.662813e+06                ...                  \n",
       "20741      12.0    1.0  6.677245e+07                ...                  \n",
       "13103       6.0    2.0  4.389199e+06                ...                  \n",
       "\n",
       "       child_on_acc_pre_school_3,013  child_on_acc_pre_school_7,311  \\\n",
       "14065                              0                              0   \n",
       "12978                              1                              0   \n",
       "18695                              0                              0   \n",
       "26411                              0                              0   \n",
       "1419                               0                              1   \n",
       "29787                              0                              0   \n",
       "18411                              0                              0   \n",
       "11541                              1                              0   \n",
       "20741                              0                              0   \n",
       "13103                              1                              0   \n",
       "\n",
       "       modern_education_share_0  modern_education_share_90,92  \\\n",
       "14065                         0                             0   \n",
       "12978                         0                             1   \n",
       "18695                         0                             0   \n",
       "26411                         0                             0   \n",
       "1419                          1                             0   \n",
       "29787                         0                             0   \n",
       "18411                         0                             0   \n",
       "11541                         0                             1   \n",
       "20741                         0                             0   \n",
       "13103                         0                             1   \n",
       "\n",
       "       modern_education_share_93,08  modern_education_share_95,4918  \\\n",
       "14065                             1                               0   \n",
       "12978                             0                               0   \n",
       "18695                             1                               0   \n",
       "26411                             1                               0   \n",
       "1419                              0                               0   \n",
       "29787                             0                               1   \n",
       "18411                             1                               0   \n",
       "11541                             0                               0   \n",
       "20741                             1                               0   \n",
       "13103                             0                               0   \n",
       "\n",
       "       old_education_build_share_0  old_education_build_share_23,14  \\\n",
       "14065                            0                                0   \n",
       "12978                            0                                1   \n",
       "18695                            0                                0   \n",
       "26411                            0                                0   \n",
       "1419                             1                                0   \n",
       "29787                            0                                0   \n",
       "18411                            0                                0   \n",
       "11541                            0                                1   \n",
       "20741                            0                                0   \n",
       "13103                            0                                1   \n",
       "\n",
       "       old_education_build_share_25,47  old_education_build_share_8,2517  \n",
       "14065                                1                                 0  \n",
       "12978                                0                                 0  \n",
       "18695                                1                                 0  \n",
       "26411                                1                                 0  \n",
       "1419                                 0                                 0  \n",
       "29787                                0                                 1  \n",
       "18411                                1                                 0  \n",
       "11541                                0                                 0  \n",
       "20741                                1                                 0  \n",
       "13103                                0                                 0  \n",
       "\n",
       "[10 rows x 560 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's split our dataset into train 70 % and test 30% by using sklearn. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Look at first 10 rows what you get.\n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's see how much data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size = (21329, 560)\n",
      "Test size = (9142, 560)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size =\", X_train.shape)\n",
    "print(\"Test size =\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are too many features in this dataset and not all of them are equally important for our problem. Besides, using the whole dataset as-is to train a linear model will, for sure, lead to overfitting. Instead of painful and time consuming manual selection of the most relevant data, we will use the methods of automatic feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But at first, we almost forgot to take a look at our targets. Let's plot `y_train` histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1a16f4f9b0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big variance in it and it's far from being a normal distribution. In the real-world problems it happens all the time: the data can be far from perfect. We can use some tricks to make it more like what we want.\n",
    "In this particular case we can predict $\\log y$ instead of $y$. This transformation is invertible, so we will be able to get our $y$ back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1a16fcad68>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)\n",
    "y_train_log.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it looks more like the data we want to deal with.\n",
    "\n",
    "The preprocessing is finally over, so now we are ready for the actual task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center><span style=\"color:red;\">**IMPORTANT NOTICE**</span></center></h3>\n",
    "\n",
    "If you have difficulties with solving the below problems take a look at seminar $7$ on feature and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (1 pt.): Random forest feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use random forest to find the imortance of features. Plot the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "### BEGIN Solution\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore')\n",
    "reg = RandomForestRegressor(max_depth=10, random_state=0, n_estimators=100)\n",
    "reg.fit(X_train,y_train_log)\n",
    "plt.figure(figsize=(7.5,5))\n",
    "plt.hist(reg.feature_importances_, bins=100)\n",
    "plt.show()\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the 20 most important features and their **values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_sq</th>\n",
       "      <th>sport_count_3000</th>\n",
       "      <th>cafe_count_3000</th>\n",
       "      <th>cafe_count_5000_price_2500</th>\n",
       "      <th>cafe_count_2000</th>\n",
       "      <th>ttk_km</th>\n",
       "      <th>cafe_count_5000</th>\n",
       "      <th>num_room</th>\n",
       "      <th>swim_pool_km</th>\n",
       "      <th>micex_cbi_tr</th>\n",
       "      <th>exhibition_km</th>\n",
       "      <th>eurrub</th>\n",
       "      <th>metro_min_avto</th>\n",
       "      <th>metro_km_avto</th>\n",
       "      <th>kindergarten_km</th>\n",
       "      <th>public_healthcare_km</th>\n",
       "      <th>brent</th>\n",
       "      <th>zd_vokzaly_avto_km</th>\n",
       "      <th>green_zone_part</th>\n",
       "      <th>cafe_count_5000_price_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14065</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14.298225</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.106936</td>\n",
       "      <td>244.71</td>\n",
       "      <td>7.402244</td>\n",
       "      <td>47.2639</td>\n",
       "      <td>4.721045</td>\n",
       "      <td>3.776836</td>\n",
       "      <td>0.286711</td>\n",
       "      <td>5.422551</td>\n",
       "      <td>107.88</td>\n",
       "      <td>24.061214</td>\n",
       "      <td>0.055644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12978</th>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>30.010070</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.523441</td>\n",
       "      <td>241.54</td>\n",
       "      <td>18.712429</td>\n",
       "      <td>45.1999</td>\n",
       "      <td>23.409854</td>\n",
       "      <td>19.391931</td>\n",
       "      <td>0.126951</td>\n",
       "      <td>19.151443</td>\n",
       "      <td>109.70</td>\n",
       "      <td>36.868718</td>\n",
       "      <td>0.375974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18695</th>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>98</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>3.247230</td>\n",
       "      <td>333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.707560</td>\n",
       "      <td>244.85</td>\n",
       "      <td>1.235752</td>\n",
       "      <td>46.9807</td>\n",
       "      <td>1.426410</td>\n",
       "      <td>0.688338</td>\n",
       "      <td>0.685655</td>\n",
       "      <td>2.286509</td>\n",
       "      <td>110.55</td>\n",
       "      <td>9.119447</td>\n",
       "      <td>0.023464</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26411</th>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>17.148737</td>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.521691</td>\n",
       "      <td>242.15</td>\n",
       "      <td>4.555385</td>\n",
       "      <td>80.0946</td>\n",
       "      <td>2.152792</td>\n",
       "      <td>1.722233</td>\n",
       "      <td>0.897889</td>\n",
       "      <td>2.052908</td>\n",
       "      <td>61.06</td>\n",
       "      <td>25.699461</td>\n",
       "      <td>0.496315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>326</td>\n",
       "      <td>181</td>\n",
       "      <td>112</td>\n",
       "      <td>0.638719</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.501888</td>\n",
       "      <td>213.22</td>\n",
       "      <td>2.956832</td>\n",
       "      <td>38.8094</td>\n",
       "      <td>2.525458</td>\n",
       "      <td>1.521401</td>\n",
       "      <td>0.233241</td>\n",
       "      <td>0.749645</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1.902312</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       full_sq  sport_count_3000  cafe_count_3000  cafe_count_5000_price_2500  \\\n",
       "14065       46                 1                5                           1   \n",
       "12978       77                 3                6                           3   \n",
       "18695       39                29               98                          36   \n",
       "26411       52                 6                9                           1   \n",
       "1419        30                50              326                         181   \n",
       "\n",
       "       cafe_count_2000     ttk_km  cafe_count_5000  num_room  swim_pool_km  \\\n",
       "14065                2  14.298225               16       1.0      8.106936   \n",
       "12978                2  30.010070                7       3.0     22.523441   \n",
       "18695               33   3.247230              333       1.0      3.707560   \n",
       "26411                7  17.148737               20       2.0      2.521691   \n",
       "1419               112   0.638719             1244       0.0      0.501888   \n",
       "\n",
       "       micex_cbi_tr  exhibition_km   eurrub  metro_min_avto  metro_km_avto  \\\n",
       "14065        244.71       7.402244  47.2639        4.721045       3.776836   \n",
       "12978        241.54      18.712429  45.1999       23.409854      19.391931   \n",
       "18695        244.85       1.235752  46.9807        1.426410       0.688338   \n",
       "26411        242.15       4.555385  80.0946        2.152792       1.722233   \n",
       "1419         213.22       2.956832  38.8094        2.525458       1.521401   \n",
       "\n",
       "       kindergarten_km  public_healthcare_km   brent  zd_vokzaly_avto_km  \\\n",
       "14065         0.286711              5.422551  107.88           24.061214   \n",
       "12978         0.126951             19.151443  109.70           36.868718   \n",
       "18695         0.685655              2.286509  110.55            9.119447   \n",
       "26411         0.897889              2.052908   61.06           25.699461   \n",
       "1419          0.233241              0.749645  123.80            1.902312   \n",
       "\n",
       "       green_zone_part  cafe_count_5000_price_high  \n",
       "14065         0.055644                           0  \n",
       "12978         0.375974                           0  \n",
       "18695         0.023464                           1  \n",
       "26411         0.496315                           0  \n",
       "1419          0.015900                          17  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Importance, Feature label)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.3874767052191281, 'full_sq'),\n",
       " (0.04232870670615047, 'sport_count_3000'),\n",
       " (0.03458917001468469, 'cafe_count_3000'),\n",
       " (0.029250843150790566, 'cafe_count_5000_price_2500'),\n",
       " (0.023762607790253774, 'cafe_count_2000'),\n",
       " (0.010107237138299313, 'ttk_km'),\n",
       " (0.009910397163126555, 'cafe_count_5000'),\n",
       " (0.0094937474464798, 'num_room'),\n",
       " (0.008068903721671917, 'swim_pool_km'),\n",
       " (0.007987478552451723, 'micex_cbi_tr'),\n",
       " (0.007517462453607431, 'exhibition_km'),\n",
       " (0.006386840995643628, 'eurrub'),\n",
       " (0.00601926209939325, 'metro_min_avto'),\n",
       " (0.005980123692882942, 'metro_km_avto'),\n",
       " (0.005683561137005635, 'kindergarten_km'),\n",
       " (0.005353375999181145, 'public_healthcare_km'),\n",
       " (0.00533608526203448, 'brent'),\n",
       " (0.005264244624041336, 'zd_vokzaly_avto_km'),\n",
       " (0.005262679918544213, 'green_zone_part'),\n",
       " (0.004822302239678562, 'cafe_count_5000_price_high')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN Solution\n",
    "from IPython.display import display\n",
    "a = list(zip(list(reg.feature_importances_),list(X_train.keys())))\n",
    "top_20_features = sorted(a, reverse = True)[:20]\n",
    "display(X_train[list(np.array(top_20_features)[:,1])].head())\n",
    "print('(Importance, Feature label)')\n",
    "top_20_features\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (1 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On these 20 features train each of the following models\n",
    "* **Linear Regression**\n",
    "* **Ridge regression**\n",
    "* **Random forest**\n",
    "* **DecisionTree**\n",
    "\n",
    "and test its performance using the **Root Mean Squared Logarithmic Error** (RMSLE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to do it for the next tasks too, so we recommend you to implement\n",
    "a dedicated function for comparisons, which\n",
    "1. on input the function takes a training dataset `(X_train, y_train)` and a test sample `(X_test, y_test)`\n",
    "2. it trains **all of the listed models** on the `(X_train, y_train)` sample\n",
    "3. it computes and returns a table the RMSLE score of each fitted model on the test dataset`(X_test, y_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "def comparator(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "        X_train: ndarray - training inputs\n",
    "        y_train: ndarray - training targets\n",
    "        X_test: ndarray - test inputs\n",
    "        y_test: ndarray - test targets\n",
    "        \n",
    "    Returns\n",
    "    =======\n",
    "        pd.DataFrame - table of RMSLE scores of each model on test and train datasets\n",
    "    \"\"\"\n",
    "    methods = {\n",
    "        \"Linear Regression\": sklearn.linear_model.LinearRegression(), \n",
    "        \"Lasso\": linear_model.Lasso(random_state = 0), \n",
    "        \"Ridge\": linear_model.Ridge(random_state = 0),\n",
    "        \"Dtree\": sklearn.tree.DecisionTreeRegressor(random_state = 0),\n",
    "        \"RFR\": sklearn.ensemble.RandomForestRegressor(n_estimators =100,random_state = 0)\n",
    "    }\n",
    "\n",
    "### BEGIN Solution\n",
    "    error_train = []\n",
    "    error_test = []\n",
    "    for item in methods:\n",
    "        reg = methods[item]\n",
    "        reg.fit(X_train, y_train)\n",
    "        error_train.append(mean_squared_log_error(y_train, reg.predict(X_train)))\n",
    "        error_test.append(mean_squared_log_error(y_test,reg.predict(X_test)))\n",
    "                          \n",
    "### END Solution\n",
    "    return pd.DataFrame({\n",
    "        \"Methods\": list(methods.keys()),\n",
    "        \"Train loss\": error_train,\n",
    "        \"Test loss\": error_test\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Test loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.057868e-03</td>\n",
       "      <td>0.001072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>1.216642e-03</td>\n",
       "      <td>0.001196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>1.057867e-03</td>\n",
       "      <td>0.001072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dtree</td>\n",
       "      <td>8.907368e-08</td>\n",
       "      <td>0.001723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RFR</td>\n",
       "      <td>1.253526e-04</td>\n",
       "      <td>0.000871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Methods    Train loss  Test loss\n",
       "0  Linear Regression  1.057868e-03   0.001072\n",
       "1              Lasso  1.216642e-03   0.001196\n",
       "2              Ridge  1.057867e-03   0.001072\n",
       "3              Dtree  8.907368e-08   0.001723\n",
       "4                RFR  1.253526e-04   0.000871"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN Solution\n",
    "comparator(X_train[list(np.array(top_20_features)[:,1])], y_train_log, X_test[list(np.array(top_20_features)[:,1])], y_test_log)\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-backward methods\n",
    "\n",
    "The idea is to add or remove features and look how it influences the value of the loss function or some other criteria.\n",
    "\n",
    "Decision about adding or deleting a feature may be made based on:\n",
    "\n",
    "- AIC\n",
    "- BIC\n",
    "- validation error\n",
    "- Mallows $C_p$\n",
    "- sklearn's `estimator.score()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (2 pt.): Implement forward method with early stopping\n",
    "\n",
    "Implement the following greedy feature selection algorithm:\n",
    "\n",
    "```python\n",
    "# Initialize with an empty list of features.\n",
    "list_of_best_features = []\n",
    "\n",
    "while round < n_rounds:\n",
    "    round = round + 1\n",
    "    \n",
    "    if no_more_features:\n",
    "        # end loop\n",
    "\n",
    "    # Iterate over currently *unsued* features and use $k$-fold \n",
    "    # . `cross_val_score` to measure model \"quality\".\n",
    "    compute_quality_with_each_new_unused_feature(...)\n",
    "\n",
    "    # **Add** the feature that gives the highest \"quality\" of the model.\n",
    "    pick_and_add_the_best_feature(...)\n",
    "\n",
    "    if model_quality_has_increased_since_last_round:\n",
    "        round = 0\n",
    "\n",
    "return list_of_best_features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:red\">ATTN</span>**\n",
    "Use $k=3$ for the $k$-fold cv, because higher values could take a **lo-o-o-o-o-o-o-o-ong** time.\n",
    "\n",
    "Please bear in mind that **the lower** RMSLE (`mean_squared_log_error`) is, **the higher the model \"quality\" is**.\n",
    "\n",
    "Please look up `cross_val_score(...)` peculiarities in [scikit's manual](https://scikit-learn.org/stable/documentation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below implement a function that would iterate over a list of features and use $k$-fold `cross_val_score` to measure model \"quality\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_train)\n",
    "y = np.array(y_train_log)\n",
    "cv = 3\n",
    "model = sklearn.tree.DecisionTreeRegressor(random_state = 0)\n",
    "best_indices = []\n",
    "scores = {}\n",
    "indices = [i for i in range(X.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index in indices:\n",
    "        if index not in best_indices:\n",
    "            scores[index] = -cross_val_score(model,X[:,best_indices+[index]],y, cv = cv, scoring = 'neg_mean_squared_log_error').mean()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.0010667494118472044,\n",
       " 1: 0.0012206677157977344,\n",
       " 2: 0.0013464144114706215,\n",
       " 3: 0.0013193446483073003,\n",
       " 4: 0.001336294830844122,\n",
       " 5: 0.001302342334573038,\n",
       " 6: 0.0011740330126178934,\n",
       " 7: 0.00130012954018313,\n",
       " 8: 0.0013339497602017593,\n",
       " 9: 0.0011907852417974343,\n",
       " 10: 0.0011907607298848706,\n",
       " 11: 0.0011908274701389967,\n",
       " 12: 0.0012077774077682562,\n",
       " 13: 0.0011907662348018908,\n",
       " 14: 0.0012285748080005615,\n",
       " 15: 0.0013141419398244381,\n",
       " 16: 0.0011907662348018906,\n",
       " 17: 0.0012205000608302553,\n",
       " 18: 0.0013005718350789554,\n",
       " 19: 0.0013610394689641998,\n",
       " 20: 0.0012770167056604142,\n",
       " 21: 0.0013284274132249363,\n",
       " 22: 0.0013384196061664785,\n",
       " 23: 0.0012804430663094857,\n",
       " 24: 0.0013322517507586065,\n",
       " 25: 0.0013431643789853821,\n",
       " 26: 0.0013099677005682074,\n",
       " 27: 0.0012855481655216247,\n",
       " 28: 0.0011907740342996683,\n",
       " 29: 0.0011907671478646185,\n",
       " 30: 0.0011907740342996678,\n",
       " 31: 0.0011907662348018908,\n",
       " 32: 0.0011907602253367294,\n",
       " 33: 0.0011927058598213195,\n",
       " 34: 0.0011966183789414294,\n",
       " 35: 0.0011907784208126503,\n",
       " 36: 0.0011907662348018906,\n",
       " 37: 0.0011907662348018906,\n",
       " 38: 0.0011908117150982813,\n",
       " 39: 0.0011907662348018906,\n",
       " 40: 0.0011907662348018908,\n",
       " 41: 0.00119408413792147,\n",
       " 42: 0.0011909099155467402,\n",
       " 43: 0.0011907662348018906,\n",
       " 44: 0.0011958299375517984,\n",
       " 45: 0.0011911063035627497,\n",
       " 46: 0.001191811523078244,\n",
       " 47: 0.0011907662348018906,\n",
       " 48: 0.0011907662348018908,\n",
       " 49: 0.001190767147864618,\n",
       " 50: 0.0011907910824865543,\n",
       " 51: 0.0011907740342996678,\n",
       " 52: 0.0011907662348018906,\n",
       " 53: 0.0011902327120788417,\n",
       " 54: 0.0011907662348018908,\n",
       " 55: 0.0012355902590795956,\n",
       " 56: 0.0012700663724169517,\n",
       " 57: 0.0013310355721187035,\n",
       " 58: 0.0013437084245132098,\n",
       " 59: 0.0012448210341778657,\n",
       " 60: 0.0012857028949173975,\n",
       " 61: 0.0012441260747950399,\n",
       " 62: 0.00136918092296596,\n",
       " 63: 0.0013369328645127816,\n",
       " 64: 0.0013552539755667798,\n",
       " 65: 0.00122960236511477,\n",
       " 66: 0.0013163786196272729,\n",
       " 67: 0.0013112849669629997,\n",
       " 68: 0.0012398951857708139,\n",
       " 69: 0.0012447926629774895,\n",
       " 70: 0.001258005866970926,\n",
       " 71: 0.001203790132286844,\n",
       " 72: 0.0019137762766349576,\n",
       " 73: 0.0019256388428499308,\n",
       " 74: 0.001882621110459599,\n",
       " 75: 0.0018821823973203002,\n",
       " 76: 0.0019247608839724785,\n",
       " 77: 0.0018848494542939263,\n",
       " 78: 0.0019422603030902158,\n",
       " 79: 0.0019641645883129396,\n",
       " 80: 0.0018998681062810524,\n",
       " 81: 0.0019243083747492163,\n",
       " 82: 0.0019343238492239938,\n",
       " 83: 0.001939725578978272,\n",
       " 84: 0.001948579925024391,\n",
       " 85: 0.001948579925024391,\n",
       " 86: 0.0012349728926059303,\n",
       " 87: 0.001975773432637834,\n",
       " 88: 0.0019419526686885724,\n",
       " 89: 0.0012169161939384503,\n",
       " 90: 0.001970217040558331,\n",
       " 91: 0.0019690624536441437,\n",
       " 92: 0.0019280190008958535,\n",
       " 93: 0.001879064439650494,\n",
       " 94: 0.001893529924569382,\n",
       " 95: 0.0018794567916455987,\n",
       " 96: 0.001888628721018662,\n",
       " 97: 0.0018904935991997443,\n",
       " 98: 0.0019251537910779275,\n",
       " 99: 0.001279124886969292,\n",
       " 100: 0.0019121715273099284,\n",
       " 101: 0.00126207533765507,\n",
       " 102: 0.001886031119132528,\n",
       " 103: 0.0018900325172587116,\n",
       " 104: 0.001356040758684238,\n",
       " 105: 0.0018844656765100102,\n",
       " 106: 0.0013219410650427803,\n",
       " 107: 0.0019197692694209994,\n",
       " 108: 0.0019254757173064706,\n",
       " 109: 0.0019158561318070614,\n",
       " 110: 0.0019075730842508978,\n",
       " 111: 0.0019094993842229794,\n",
       " 112: 0.0018933917475871178,\n",
       " 113: 0.0018878320764153772,\n",
       " 114: 0.0019552661593183226,\n",
       " 115: 0.0019112291151090245,\n",
       " 116: 0.001940269128936502,\n",
       " 117: 0.0019241125510420971,\n",
       " 118: 0.001908088859418652,\n",
       " 119: 0.0019237408079530126,\n",
       " 120: 0.001966256968262784,\n",
       " 121: 0.0019103822640844294,\n",
       " 122: 0.0019534057112274472,\n",
       " 123: 0.0019297717154084864,\n",
       " 124: 0.0019142541723440852,\n",
       " 125: 0.001898448521626313,\n",
       " 126: 0.0019231135930157614,\n",
       " 127: 0.001922423957560367,\n",
       " 128: 0.0019176261047284457,\n",
       " 129: 0.0018802985030560854,\n",
       " 130: 0.0019350520478908038,\n",
       " 131: 0.001897862226873372,\n",
       " 132: 0.001923100657878957,\n",
       " 133: 0.0018691128859056952,\n",
       " 134: 0.001941040495339441,\n",
       " 135: 0.0018945322670239739,\n",
       " 136: 0.0015574216723892927,\n",
       " 137: 0.0015226882155711715,\n",
       " 138: 0.0013233258419599002,\n",
       " 139: 0.001381123203382745,\n",
       " 140: 0.0013525782935604688,\n",
       " 141: 0.0013508390209540574,\n",
       " 142: 0.0013103584823516998,\n",
       " 143: 0.0013352189215622963,\n",
       " 144: 0.0013262187465361678,\n",
       " 145: 0.001356277961989982,\n",
       " 146: 0.0013462714026273338,\n",
       " 147: 0.0013358586459728836,\n",
       " 148: 0.0013272212440431175,\n",
       " 149: 0.0013247564872143497,\n",
       " 150: 0.0013267885152407032,\n",
       " 151: 0.0013537396576097094,\n",
       " 152: 0.0013670352385975297,\n",
       " 153: 0.0013484571003717308,\n",
       " 154: 0.0013528803622673343,\n",
       " 155: 0.0013691165918016032,\n",
       " 156: 0.001358990247765245,\n",
       " 157: 0.0013413867726309123,\n",
       " 158: 0.0013663243391626577,\n",
       " 159: 0.001581798742984832,\n",
       " 160: 0.0015608845993419914,\n",
       " 161: 0.00131219754024487,\n",
       " 162: 0.0014262907048893806,\n",
       " 163: 0.0013332812943400209,\n",
       " 164: 0.0013119067870370205,\n",
       " 165: 0.0012977224518966276,\n",
       " 166: 0.001378902036691701,\n",
       " 167: 0.0013790832152251059,\n",
       " 168: 0.0014329015458232968,\n",
       " 169: 0.0013353425138247649,\n",
       " 170: 0.0013088588237332311,\n",
       " 171: 0.0012956173672279755,\n",
       " 172: 0.0012984533997196163,\n",
       " 173: 0.0013000782360170617,\n",
       " 174: 0.0013383937705199668,\n",
       " 175: 0.0013554293739962793,\n",
       " 176: 0.001331584439234531,\n",
       " 177: 0.0013354144566550402,\n",
       " 178: 0.00136298327214077,\n",
       " 179: 0.0013438739617939938,\n",
       " 180: 0.0013113344459110088,\n",
       " 181: 0.0013619213754115088,\n",
       " 182: 0.0016082643768860906,\n",
       " 183: 0.001583708706579186,\n",
       " 184: 0.001300983331802451,\n",
       " 185: 0.0014137314007557685,\n",
       " 186: 0.0013055571682440207,\n",
       " 187: 0.0013280735137922047,\n",
       " 188: 0.0013019818070869544,\n",
       " 189: 0.0014318126228715332,\n",
       " 190: 0.0013790458928898454,\n",
       " 191: 0.0014536800484397745,\n",
       " 192: 0.001314801978898188,\n",
       " 193: 0.0012999108013678092,\n",
       " 194: 0.0012796069161680266,\n",
       " 195: 0.0012934138376348818,\n",
       " 196: 0.0012928044801538774,\n",
       " 197: 0.0013330124244811305,\n",
       " 198: 0.0013310704281215297,\n",
       " 199: 0.0013235408735520432,\n",
       " 200: 0.001324814528407408,\n",
       " 201: 0.0013595662338463444,\n",
       " 202: 0.001341375133309438,\n",
       " 203: 0.0012937154696896326,\n",
       " 204: 0.0013542400298777832,\n",
       " 205: 0.0016126281065556736,\n",
       " 206: 0.0016107382534147048,\n",
       " 207: 0.0013038384691374312,\n",
       " 208: 0.001450936475349021,\n",
       " 209: 0.001296938914787222,\n",
       " 210: 0.0013638835739317133,\n",
       " 211: 0.0013114467588522226,\n",
       " 212: 0.0014969047813368375,\n",
       " 213: 0.00140782129243757,\n",
       " 214: 0.001550663607123944,\n",
       " 215: 0.001304325810928971,\n",
       " 216: 0.001293407747057232,\n",
       " 217: 0.0012750273180379592,\n",
       " 218: 0.0012893398346630984,\n",
       " 219: 0.0012687092175855156,\n",
       " 220: 0.001317383620584931,\n",
       " 221: 0.001322486044576792,\n",
       " 222: 0.001308365976228025,\n",
       " 223: 0.0013166470021826325,\n",
       " 224: 0.0013629278607388438,\n",
       " 225: 0.001329254124682066,\n",
       " 226: 0.0012672161689426269,\n",
       " 227: 0.0013483320752419095,\n",
       " 228: 0.001621814760222306,\n",
       " 229: 0.0015416762587756436,\n",
       " 230: 0.0013089460672164853,\n",
       " 231: 0.0015236788158979012,\n",
       " 232: 0.0012825128779952904,\n",
       " 233: 0.001447354809729898,\n",
       " 234: 0.001310948539195671,\n",
       " 235: 0.0015577353757564383,\n",
       " 236: 0.001492303232229,\n",
       " 237: 0.0016175714247177566,\n",
       " 238: 0.0012956923052375754,\n",
       " 239: 0.0013002335751319026,\n",
       " 240: 0.0012742359958306047,\n",
       " 241: 0.0012826041381005639,\n",
       " 242: 0.0012753688120057019,\n",
       " 243: 0.0013128923346954584,\n",
       " 244: 0.0012978308013876754,\n",
       " 245: 0.0013048960602197273,\n",
       " 246: 0.001296214233288794,\n",
       " 247: 0.0013566676085013626,\n",
       " 248: 0.001302773101424439,\n",
       " 249: 0.0012564301355351799,\n",
       " 250: 0.0013319659076491287,\n",
       " 251: 0.001548183925258232,\n",
       " 252: 0.0014999083278370796,\n",
       " 253: 0.0013434700887632444,\n",
       " 254: 0.0015694759832539965,\n",
       " 255: 0.001265121431194812,\n",
       " 256: 0.0015360916039974697,\n",
       " 257: 0.0013494015326983378,\n",
       " 258: 0.0017290591455274903,\n",
       " 259: 0.0016161966059231845,\n",
       " 260: 0.0017082958013741895,\n",
       " 261: 0.0012878371584852876,\n",
       " 262: 0.001308314781234103,\n",
       " 263: 0.001262530976554969,\n",
       " 264: 0.0012818616063042154,\n",
       " 265: 0.0012946602470862727,\n",
       " 266: 0.0012855014046934323,\n",
       " 267: 0.0012729782259238464,\n",
       " 268: 0.001279090643809247,\n",
       " 269: 0.001289563615111988,\n",
       " 270: 0.0013515851279810606,\n",
       " 271: 0.0012824272057398152,\n",
       " 272: 0.0012652915344013235,\n",
       " 273: 0.0013250381914263973,\n",
       " 274: 0.0013385246483529642,\n",
       " 275: 0.001336975642604213,\n",
       " 276: 0.001340071179113234,\n",
       " 277: 0.0013385246483529646,\n",
       " 278: 0.0013385246483529646,\n",
       " 279: 0.0013374045756477184,\n",
       " 280: 0.0013385246483529649,\n",
       " 281: 0.0013369756426042127,\n",
       " 282: 0.0014409696781027502,\n",
       " 283: 0.001441449051187924,\n",
       " 284: 0.001423383793281599,\n",
       " 285: 0.0013385246483529646,\n",
       " 286: 0.0013374045756477184,\n",
       " 287: 0.0013374045756477202,\n",
       " 288: 0.0013378936688869935,\n",
       " 289: 0.001341459894711465,\n",
       " 290: 0.0014380358533784825,\n",
       " 291: 0.0014388017569964197,\n",
       " 292: 0.0014139912124392928,\n",
       " 293: 0.0014211341378726119,\n",
       " 294: 0.0013396528732054526,\n",
       " 295: 0.0013396528732054532,\n",
       " 296: 0.0013406031559743306,\n",
       " 297: 0.0013385246483529642,\n",
       " 298: 0.0013385246483529649,\n",
       " 299: 0.0013425842528829862,\n",
       " 300: 0.0013374045756477189,\n",
       " 301: 0.0013374045756477197,\n",
       " 302: 0.001338524648352964,\n",
       " 303: 0.0013374045756477234,\n",
       " 304: 0.0013374045756477184,\n",
       " 305: 0.0013374045756477197,\n",
       " 306: 0.0013385246483529646,\n",
       " 307: 0.001337404575647719,\n",
       " 308: 0.001337404575647719,\n",
       " 309: 0.0013374045756477197,\n",
       " 310: 0.001337404575647719,\n",
       " 311: 0.0013374045756477197,\n",
       " 312: 0.001337404575647719,\n",
       " 313: 0.0013374045756477184,\n",
       " 314: 0.0013374045756477184,\n",
       " 315: 0.0013374045756477202,\n",
       " 316: 0.0013374045756477189,\n",
       " 317: 0.0013374045756477189,\n",
       " 318: 0.0013374045756477189,\n",
       " 319: 0.0013374045756477202,\n",
       " 320: 0.0013374045756477215,\n",
       " 321: 0.0013411465679838655,\n",
       " 322: 0.0013374045756477189,\n",
       " 323: 0.00133740457564772,\n",
       " 324: 0.0013374045756477193,\n",
       " 325: 0.0013374045756477197,\n",
       " 326: 0.0013374045756477197,\n",
       " 327: 0.001355264169360077,\n",
       " 328: 0.0013458956865070777,\n",
       " 329: 0.0013374045756477189,\n",
       " 330: 0.0013374045756477197,\n",
       " 331: 0.001355264169360078,\n",
       " 332: 0.001355264169360078,\n",
       " 333: 0.0013405294623105243,\n",
       " 334: 0.0013374045756477202,\n",
       " 335: 0.0013435073593426091,\n",
       " 336: 0.0013374045756477215,\n",
       " 337: 0.001355264169360078,\n",
       " 338: 0.0013552641693600784,\n",
       " 339: 0.001337404575647719,\n",
       " 340: 0.00133740457564772,\n",
       " 341: 0.0013374045756477197,\n",
       " 342: 0.0013374045756477202,\n",
       " 343: 0.0013385246483529646,\n",
       " 344: 0.0013385613192346726,\n",
       " 345: 0.0013382864873297065,\n",
       " 346: 0.0013391829586915725,\n",
       " 347: 0.0013381940254317132,\n",
       " 348: 0.0013385059914119309,\n",
       " 349: 0.0013395911296515482,\n",
       " 350: 0.001339788036711752,\n",
       " 351: 0.0013374045756477193,\n",
       " 352: 0.0013374045756477226,\n",
       " 353: 0.0013374045756477202,\n",
       " 354: 0.00133740457564772,\n",
       " 355: 0.0013374045756477202,\n",
       " 356: 0.0013374045756477202,\n",
       " 357: 0.0013374045756477202,\n",
       " 358: 0.0013374045756477202,\n",
       " 359: 0.0013487496204284887,\n",
       " 360: 0.001363444401446409,\n",
       " 361: 0.0013374045756477184,\n",
       " 362: 0.001339788036711752,\n",
       " 363: 0.0013374045756477184,\n",
       " 364: 0.001339788036711752,\n",
       " 365: 0.0013374045756477184,\n",
       " 366: 0.0013374045756477189,\n",
       " 367: 0.001339788036711752,\n",
       " 368: 0.001337404575647719,\n",
       " 369: 0.0013374045756477184,\n",
       " 370: 0.0013663567974864829,\n",
       " 371: 0.0013663567974864894,\n",
       " 372: 0.0013676445724909846,\n",
       " 373: 0.0013681417076830198,\n",
       " 374: 0.0013697946353090566,\n",
       " 375: 0.0013690837432521793,\n",
       " 376: 0.0013686430070761486,\n",
       " 377: 0.0013697072683368698,\n",
       " 378: 0.0013672367975683648,\n",
       " 379: 0.0013685761890739967,\n",
       " 380: 0.0013693650503185009,\n",
       " 381: 0.001369823386090567,\n",
       " 382: 0.0013685044088068506,\n",
       " 383: 0.0013692399886079726,\n",
       " 384: 0.0013695672003657573,\n",
       " 385: 0.0013700705566515297,\n",
       " 386: 0.0013696634208728096,\n",
       " 387: 0.0013692174378118224,\n",
       " 388: 0.0013692957687852295,\n",
       " 389: 0.001369844551241589,\n",
       " 390: 0.0013697188626794075,\n",
       " 391: 0.0013695760565201195,\n",
       " 392: 0.001367422916977522,\n",
       " 393: 0.0013690486088091943,\n",
       " 394: 0.001368280842242639,\n",
       " 395: 0.001367112607311567,\n",
       " 396: 0.0013692031119476565,\n",
       " 397: 0.0013698811701209962,\n",
       " 398: 0.0013680279568201812,\n",
       " 399: 0.0013678673751793236,\n",
       " 400: 0.0013694131027026154,\n",
       " 401: 0.0013609339339017848,\n",
       " 402: 0.0013696601150978169,\n",
       " 403: 0.001367382079501239,\n",
       " 404: 0.0013694151680908265,\n",
       " 405: 0.001369431737173013,\n",
       " 406: 0.0013691865160912718,\n",
       " 407: 0.001368454186831213,\n",
       " 408: 0.001369751784412829,\n",
       " 409: 0.0013692787315446164,\n",
       " 410: 0.0013697473226990575,\n",
       " 411: 0.0013703265173009057,\n",
       " 412: 0.0013697397512518127,\n",
       " 413: 0.0013683452106376711,\n",
       " 414: 0.0013694150654716339,\n",
       " 415: 0.0013678601803502211,\n",
       " 416: 0.0013700277052684977,\n",
       " 417: 0.0013695439467079965,\n",
       " 418: 0.0013698981714290379,\n",
       " 419: 0.0013685344164400283,\n",
       " 420: 0.0013691825156049527,\n",
       " 421: 0.0013661342027087853,\n",
       " 422: 0.0013694687697893345,\n",
       " 423: 0.0013655076621241656,\n",
       " 424: 0.0013694627589423342,\n",
       " 425: 0.001369742722910193,\n",
       " 426: 0.0013689049976064415,\n",
       " 427: 0.001369850939985138,\n",
       " 428: 0.0013694959252043557,\n",
       " 429: 0.0013621759607969602,\n",
       " 430: 0.0013696682551238466,\n",
       " 431: 0.0013690177326288165,\n",
       " 432: 0.0013689969840181854,\n",
       " 433: 0.0013699006907826078,\n",
       " 434: 0.001368384169076607,\n",
       " 435: 0.0013685195905875555,\n",
       " 436: 0.0013696373302385975,\n",
       " 437: 0.0013617031280838028,\n",
       " 438: 0.0013701527244926476,\n",
       " 439: 0.0013695131785809629,\n",
       " 440: 0.0013700161301552101,\n",
       " 441: 0.0013695570629599313,\n",
       " 442: 0.0013668035279319583,\n",
       " 443: 0.0013657461930663833,\n",
       " 444: 0.0013662941135813688,\n",
       " 445: 0.0013699422440307963,\n",
       " 446: 0.0013697487148788795,\n",
       " 447: 0.00136951339467158,\n",
       " 448: 0.0013695976920475338,\n",
       " 449: 0.0013636682233221242,\n",
       " 450: 0.0013696905533362128,\n",
       " 451: 0.001369684980977342,\n",
       " 452: 0.00136999041934533,\n",
       " 453: 0.0013693338453259248,\n",
       " 454: 0.0013699513226336387,\n",
       " 455: 0.0013692396488076963,\n",
       " 456: 0.00136922098670615,\n",
       " 457: 0.0013685929118192428,\n",
       " 458: 0.0013651384179947281,\n",
       " 459: 0.0013455774791884245,\n",
       " 460: 0.001370041913384322,\n",
       " 461: 0.0013696656183585516,\n",
       " 462: 0.0013690039048600017,\n",
       " 463: 0.0013683568695004149,\n",
       " 464: 0.0013695525396851844,\n",
       " 465: 0.0013696669809342644,\n",
       " 466: 0.0013683785296747287,\n",
       " 467: 0.0013694829374325285,\n",
       " 468: 0.0013573522868548257,\n",
       " 469: 0.0013682459213446025,\n",
       " 470: 0.0013695625925565734,\n",
       " 471: 0.001369709766038514,\n",
       " 472: 0.0013696656183585516,\n",
       " 473: 0.001360985457816331,\n",
       " 474: 0.0013631897703545645,\n",
       " 475: 0.0013697300122085213,\n",
       " 476: 0.0013694006064788819,\n",
       " 477: 0.0013681756224118108,\n",
       " 478: 0.0013698058794541547,\n",
       " 479: 0.0013646661967710498,\n",
       " 480: 0.001367501706618668,\n",
       " 481: 0.0013624145918742444,\n",
       " 482: 0.0013698697850742687,\n",
       " 483: 0.0013699018815511357,\n",
       " 484: 0.0013677114753184894,\n",
       " 485: 0.0013698061799836795,\n",
       " 486: 0.001369652267485091,\n",
       " 487: 0.0013695945064514283,\n",
       " 488: 0.0013695651049943438,\n",
       " 489: 0.0013702915707329106,\n",
       " 490: 0.0013689709163268093,\n",
       " 491: 0.0013692349774256504,\n",
       " 492: 0.0013701034000333115,\n",
       " 493: 0.001369510306459013,\n",
       " 494: 0.0013674721477770111,\n",
       " 495: 0.0013698349153819268,\n",
       " 496: 0.0013686790107328456,\n",
       " 497: 0.0013682540344699208,\n",
       " 498: 0.0013676954594220153,\n",
       " 499: 0.0013695443765401937,\n",
       " 500: 0.001369420518881605,\n",
       " 501: 0.0013697077651323112,\n",
       " 502: 0.0013691214602135713,\n",
       " 503: 0.001369490290479317,\n",
       " 504: 0.0013688126260765158,\n",
       " 505: 0.0013662274541630355,\n",
       " 506: 0.0013696382981590787,\n",
       " 507: 0.0013693655787102629,\n",
       " 508: 0.0013693214700735347,\n",
       " 509: 0.001369476191730272,\n",
       " 510: 0.0013696610579118683,\n",
       " 511: 0.0013699906322792931,\n",
       " 512: 0.0013686542456018645,\n",
       " 513: 0.0013697423621153598,\n",
       " 514: 0.0013663037944940369,\n",
       " 515: 0.001367132664883124,\n",
       " 516: 0.0013698180097133465,\n",
       " 517: 0.00136956193580924,\n",
       " 518: 0.0013560247820371641,\n",
       " 519: 0.00135602478203716,\n",
       " 520: 0.001369075545603543,\n",
       " 521: 0.0013690755456035395,\n",
       " 522: 0.0013667628153783997,\n",
       " 523: 0.0013667628153783988,\n",
       " 524: 0.001368961190192668,\n",
       " 525: 0.0013689611901926673,\n",
       " 526: 0.0013623267956197928,\n",
       " 527: 0.0013623267956197947,\n",
       " 528: 0.001367605747996474,\n",
       " 529: 0.0013676057479964704,\n",
       " 530: 0.0013660895873483937,\n",
       " 531: 0.0013660895873483896,\n",
       " 532: 0.0013694849801961248,\n",
       " 533: 0.0013694849801961235,\n",
       " 534: 0.0013690747680380627,\n",
       " 535: 0.0013690747680380588,\n",
       " 536: 0.0013674847288263207,\n",
       " 537: 0.0013674847288263194,\n",
       " 538: 0.001367796842324972,\n",
       " 539: 0.0013677968423249705,\n",
       " 540: 0.0013688425443880506,\n",
       " 541: 0.001368842544388049,\n",
       " 542: 0.0013628176515667594,\n",
       " 543: 0.0013691986905172025,\n",
       " 544: 0.001319267982880989,\n",
       " 545: 0.0013652906971323344,\n",
       " 546: 0.0013613888235090282,\n",
       " 547: 0.0013606999829834908,\n",
       " 548: 0.001363444401446409,\n",
       " 549: 0.0013604865159376857,\n",
       " 550: 0.00136707376673047,\n",
       " 551: 0.0013563291226608447,\n",
       " 552: 0.0013498610931670788,\n",
       " 553: 0.00136707376673047,\n",
       " 554: 0.0013604865159376857,\n",
       " 555: 0.0013606999829834908,\n",
       " 556: 0.0013498610931670788,\n",
       " 557: 0.00136707376673047,\n",
       " 558: 0.0013604865159376857,\n",
       " 559: 0.0013606999829834908}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_min = [min(scores.keys(), key=(lambda k: scores[k])),min(scores.values())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(key_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def selection_step(model, X, y, used_features=(), cv=3):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "        X: ndarray - training inputs\n",
    "        y: ndarray - training targets\n",
    "        used_features: - list of features\n",
    "        cv: int - number of folds\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "        scores - dictionary of scores\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    ### BEGIN Solution\n",
    "    indices = [i for i in range(X.shape[1])]\n",
    "    for index in indices:\n",
    "        if index not in best_indices:\n",
    "            scores[index] = -cross_val_score(model,X[:,used_features+[index]],y, cv = cv, scoring = 'neg_mean_squared_log_error').mean()\n",
    "    ### END Solution\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_steps(X, y, n_rounds, method):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "        X: ndarray - training inputs\n",
    "        y: ndarray - training targets\n",
    "        n_rounds: int - early stop when score doesn't increase n_rounds\n",
    "        method: sklearn model\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "        feat_best_list - list of features\n",
    "    \"\"\"\n",
    "    \n",
    "    feat_best_list = []\n",
    "\n",
    "    ### BEGIN Solution\n",
    "    min_error_value = np.inf\n",
    "    rnd = 0\n",
    "    while rnd < n_rounds:\n",
    "        if len(feat_best_list) == X.shape[1]:\n",
    "            break\n",
    "        else:\n",
    "            scores = selection_step(method, X, y, feat_best_list)\n",
    "            min_error_value_current = [min(scores.keys(), key=(lambda k: scores[k])),min(scores.values())]\n",
    "            if min_error_value_current[1] <= min_error_value:\n",
    "                min_error_value = min_error_value_current[1]\n",
    "                feat_best_list.append(min_error_value_current[0])\n",
    "                rnd = 0\n",
    "                #print(feat_best_list, 'here', rnd, ' score', min_error_value)\n",
    "            else:\n",
    "                rnd += 1\n",
    "                feat_best_list.append(min_error_value_current[0])\n",
    "                #print(feat_best_list, 'there', rnd, 'score', min_error_value)\n",
    "    ### END Solution\n",
    "    \n",
    "    return feat_best_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function implemented above and use DecisionTreeRegressor to get the best features according to this algorithm and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_sq</th>\n",
       "      <th>ecology_no data</th>\n",
       "      <th>sub_area_Nekrasovka</th>\n",
       "      <th>sub_area_Poselenie Vnukovskoe</th>\n",
       "      <th>sub_area_Poselenie Novofedorovskoe</th>\n",
       "      <th>sub_area_Poselenie Filimonkovskoe</th>\n",
       "      <th>sub_area_Zapadnoe Degunino</th>\n",
       "      <th>sub_area_Krylatskoe</th>\n",
       "      <th>sub_area_Hamovniki</th>\n",
       "      <th>sub_area_Poselenie Krasnopahorskoe</th>\n",
       "      <th>...</th>\n",
       "      <th>sub_area_Sokol'niki</th>\n",
       "      <th>sub_area_Birjulevo Zapadnoe</th>\n",
       "      <th>sub_area_Poselenie Moskovskij</th>\n",
       "      <th>sub_area_Poselenie Kokoshkino</th>\n",
       "      <th>sub_area_Begovoe</th>\n",
       "      <th>sub_area_Arbat</th>\n",
       "      <th>sub_area_Poselenie Shherbinka</th>\n",
       "      <th>sub_area_Ostankinskoe</th>\n",
       "      <th>sub_area_Poselenie Shherbinka</th>\n",
       "      <th>sub_area_Sokol'niki</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14065</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12978</th>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18695</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26411</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       full_sq  ecology_no data  sub_area_Nekrasovka  \\\n",
       "14065       46                0                    1   \n",
       "12978       77                1                    0   \n",
       "18695       39                0                    0   \n",
       "26411       52                1                    0   \n",
       "1419        30                0                    0   \n",
       "\n",
       "       sub_area_Poselenie Vnukovskoe  sub_area_Poselenie Novofedorovskoe  \\\n",
       "14065                              0                                   0   \n",
       "12978                              0                                   0   \n",
       "18695                              0                                   0   \n",
       "26411                              1                                   0   \n",
       "1419                               0                                   0   \n",
       "\n",
       "       sub_area_Poselenie Filimonkovskoe  sub_area_Zapadnoe Degunino  \\\n",
       "14065                                  0                           0   \n",
       "12978                                  0                           0   \n",
       "18695                                  0                           0   \n",
       "26411                                  0                           0   \n",
       "1419                                   0                           0   \n",
       "\n",
       "       sub_area_Krylatskoe  sub_area_Hamovniki  \\\n",
       "14065                    0                   0   \n",
       "12978                    0                   0   \n",
       "18695                    0                   0   \n",
       "26411                    0                   0   \n",
       "1419                     0                   0   \n",
       "\n",
       "       sub_area_Poselenie Krasnopahorskoe         ...           \\\n",
       "14065                                   0         ...            \n",
       "12978                                   0         ...            \n",
       "18695                                   0         ...            \n",
       "26411                                   0         ...            \n",
       "1419                                    0         ...            \n",
       "\n",
       "       sub_area_Sokol'niki  sub_area_Birjulevo Zapadnoe  \\\n",
       "14065                    0                            0   \n",
       "12978                    0                            0   \n",
       "18695                    0                            0   \n",
       "26411                    0                            0   \n",
       "1419                     0                            0   \n",
       "\n",
       "       sub_area_Poselenie Moskovskij  sub_area_Poselenie Kokoshkino  \\\n",
       "14065                              0                              0   \n",
       "12978                              0                              0   \n",
       "18695                              0                              0   \n",
       "26411                              0                              0   \n",
       "1419                               0                              0   \n",
       "\n",
       "       sub_area_Begovoe  sub_area_Arbat  sub_area_Poselenie Shherbinka  \\\n",
       "14065                 0               0                              0   \n",
       "12978                 0               0                              0   \n",
       "18695                 0               0                              0   \n",
       "26411                 0               0                              0   \n",
       "1419                  0               0                              0   \n",
       "\n",
       "       sub_area_Ostankinskoe  sub_area_Poselenie Shherbinka  \\\n",
       "14065                      0                              0   \n",
       "12978                      0                              0   \n",
       "18695                      0                              0   \n",
       "26411                      0                              0   \n",
       "1419                       0                              0   \n",
       "\n",
       "       sub_area_Sokol'niki  \n",
       "14065                    0  \n",
       "12978                    0  \n",
       "18695                    0  \n",
       "26411                    0  \n",
       "1419                     0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN Solution\n",
    "best_features = forward_steps(np.array(X_train),np.array(y_train_log), 2 ,sklearn.tree.DecisionTreeRegressor(random_state = 0))\n",
    "X_train[X_train.columns.values[best_features]].head()\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Linear Regression, Ridge regression, Random forest and DecisionTree to get the RMSLE score using these features. Remember the function you wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Test loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.001252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dtree</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.000936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RFR</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.000927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Methods  Train loss  Test loss\n",
       "0  Linear Regression    0.001155   0.001119\n",
       "1              Lasso    0.001277   0.001252\n",
       "2              Ridge    0.001155   0.001119\n",
       "3              Dtree    0.000883   0.000936\n",
       "4                RFR    0.000886   0.000927"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN Solution\n",
    "comparator(X_train[X_train.columns.values[best_features]], y_train_log, X_test[X_test.columns.values[best_features]], y_test_log)\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting: gradient boosting, adaboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you are asked to implement a boosting algorithm, and compare speed of\n",
    "different popular boosting libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (2 pt.): Boosting Classification on a toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a toy dataset for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=300, shuffle=True, noise=0.05, random_state=1011)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is:\n",
    "1. Implement gradient boosting algorithms with **logistic loss**\n",
    "and labels $y\\in \\{-1, +1\\}$;\n",
    "2. **Plot the decision boundary** on a $2$-d grid; \n",
    "3. Estimate the accuracy **score** on the test dataset, as well\n",
    "as other classification metrics, that you can think of;\n",
    "    \n",
    "For basic implementation please refer to seminars $8-9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 (1 pt.): Measuring the Speed and Performance\n",
    "\n",
    "Please make sure to install the following powerful packages for boosting:\n",
    "* [xgboost](https://anaconda.org/conda-forge/xgboost)\n",
    "* [lightgbm](https://anaconda.org/conda-forge/lightgbm)\n",
    "* [catboost](https://tech.yandex.com/catboost/doc/dg/concepts/python-installation-docpage/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you are asked to compare the **training time** of the **GBDT**, the\n",
    "Gradient Boosted Decision Trees, as implemeted by different popular ML libraries.\n",
    "The dataset you shall use is the [UCI Breast Cancer dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).\n",
    "You should study the parameters of each library and establish the **correspondence**\n",
    "between them.\n",
    "\n",
    "The plan is as follows:\n",
    "1. Take the **default** parameter settings, measure the training time, and plot\n",
    "the ROC curves;\n",
    "2. Use grid search with the $3$-fold cross valiadation to choose the best model.\n",
    "Then measure the training time as a function of (separately) **tree depth** and **the\n",
    "number of estimators in the ensemble**, finally **plot the ROC** curves of the best\n",
    "models.\n",
    "\n",
    "You need to make sure that you are comparing **comparable** classifiers, i.e. with\n",
    "**the same tree and ensemble hyperparameters**.\n",
    "\n",
    "<span style=\"color:green\">**NOTE**</span> You need figure out how to make parameter settings\n",
    "compatible. One possible way to understand the correspondence is to study the docs. You may\n",
    "choose the default parameters from any library.\n",
    "\n",
    "Please plot **three** ROC curves, one per library, on the same **one plot**\n",
    "with a *comprehensible [legend](https://matplotlib.org/users/legend_guide.html)*.\n",
    "\n",
    "A useful command for timing is IPython's [**timeit** cell magic](http://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,\n",
    "                                                    random_state=0x0BADBEEF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 (1 pt.): Activation functions\n",
    "Plot the following [activation functions](https://pytorch.org/docs/master/nn.html#non-linear-activation-functions) using their PyTorch realizations and their derivatives using autograd functionality:\n",
    "* ReLU, ELU ($\\alpha = 1$), Softplus ($\\beta = 1$);\n",
    "* Sign, Sigmoid, Softsign, Tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "x = torch.arange(-2, 2, .01, requires_grad=True)\n",
    "x.sum().backward() # to create x.grad\n",
    "\n",
    "f, axes = plt.subplots(2, 2, sharex=True, figsize=(15, 7))\n",
    "axes[0, 0].set_title('Values')\n",
    "axes[0, 1].set_title('Derivatives')\n",
    "\n",
    "for i, function_set in (0, (('ReLU', F.relu), ('ELU', F.elu), ('Softplus', F.softplus))), \\\n",
    "                       (1, (('Sign', torch.sign), ('Sigmoid', torch.sigmoid), ('Softsign', F.softsign), ('Tanh', torch.tanh))):\n",
    "    for function_name, activation in function_set:\n",
    "        ### BEGIN Solution\n",
    "        # ...\n",
    "        # axes[i, 0].plot('xs', 'funcion values', label=function_name)\n",
    "        # axes[i, 1].plot('xs', 'derivative values', label=function_name)\n",
    "        ### END Solution\n",
    "\n",
    "    axes[i, 0].legend()\n",
    "    axes[i, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions. Which of these functions may be, and which -- definitely are a poor choise as an activation function in a neural network? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 (3 pt.): Backpropagation\n",
    "At the seminar 10 on neural networks, we built an MLP with one hidden layer using our numpy implementations of linear layer and logistic and softmax activation functions. Your task is to\n",
    "1. implement backpropagation for these modules,\n",
    "2. train our numpy realization of MLP to classify the toy MNIST from `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits, targets = load_digits(return_X_y=True)\n",
    "digits = digits.astype(np.float32) / 255\n",
    "\n",
    "digits_train, digits_test, targets_train, targets_test = train_test_split(digits, targets, random_state=0)\n",
    "\n",
    "train_size = digits_train.shape[0]\n",
    "\n",
    "input_size = 8*8\n",
    "classes_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the MLP with backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.thetas = np.random.randn(input_size, output_size)\n",
    "        self.thetas_grads = np.empty(self.thetas)\n",
    "        self.bias = np.random.randn(output_size)\n",
    "        self.bias_grads = np.empty(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        output = np.matmul(x, self.thetas) + self.bias\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def backward(self, x, output_grad):\n",
    "        ### BEGIN Solution\n",
    "        # ... calculate grads\n",
    "        # self.thetas_grads += \n",
    "        # self.bias_grads += \n",
    "        ### END Solution\n",
    "        return input_grad\n",
    "\n",
    "\n",
    "class LogisticActivation:\n",
    "    def forward(self, x):\n",
    "        output = 1/(1 + np.exp(-x))\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward(self, x, output_grad):\n",
    "        ### BEGIN Solution\n",
    "        # ... calculate grads\n",
    "        ### END Solution\n",
    "        return input_grad\n",
    "    \n",
    "\n",
    "class SoftMaxActivation:\n",
    "    def forward(self, x):\n",
    "        output = np.exp(x) / np.exp(x).sum(axis=-1, keepdims=True)\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def backward(self, x, output_grad):\n",
    "        ### BEGIN Solution\n",
    "        # ... calculate grads\n",
    "        ### END Solution\n",
    "        return input_grad\n",
    "    \n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        self.linear1 = Linear(input_size, hidden_layer_size)\n",
    "        self.activation1 = LogisticActivation()\n",
    "        self.linear2 = Linear(hidden_layer_size, output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return linear2.forward(activation1.forward(self.linear1.forward(x)))\n",
    "\n",
    "\n",
    "    def backward(self, x, output_grad):\n",
    "        ### BEGIN Solution\n",
    "        # ... calculate and update grads\n",
    "        ### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "# Please, implement here everything else you need, like the loss function.\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "mlp = MLP(input_size=input_size, hidden_layer_size=100, output_size=classes_n)\n",
    "\n",
    "epochs_n = 200\n",
    "learning_curve = [0] * epochs_n\n",
    "test_curve = [0] * epochs_n\n",
    "\n",
    "x_train = digits_train\n",
    "x_test = digits_test\n",
    "y_train = targets_train\n",
    "y_test = targets_test\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "for epoch in range(epochs_n):\n",
    "    if epoch % 10 == 0:\n",
    "        print('Starting epoch {}'.format(epoch), end=' ')\n",
    "    for sample_i in range(train_size):\n",
    "        x = x_train[sample_i]\n",
    "        target = y_train[sample_i]\n",
    "\n",
    "        ### BEGIN Solution\n",
    "        # ... zero the gradients\n",
    "        # prediction = mlp.forward(x)\n",
    "        # loss = # use cross entropy loss\n",
    "        # learning_curve[epoch] += loss\n",
    "        # ... perform backward pass\n",
    "        # ... update the weights simply with weight -= grad * learning_rate\n",
    "    \n",
    "    # learning_curve[epoch] /= train_size\n",
    "    # prediction = mlp.forward(x_test)\n",
    "    # loss = # use cross entropy loss\n",
    "    # test_curve[epoch] = loss\n",
    "    ### END Solution\n",
    "\n",
    "\n",
    "plt.plot(learning_curve)\n",
    "plt.plot(test_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, predictions = np.max(mlp.forward(digits), -1)\n",
    "pd.DataFrame(confusion_matrix(targets, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 (3 pt.): Modelling real-life DL\n",
    "In this task you will train your own CNN for dogs vs cats classification task. The goal of this task is not to get the highest accuracy possible (try getting the highest accuracy possible though) but to model the real-life process of training a deep neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center><span style=\"color:red;\">**IMPORTANT NOTICE**</span></center></h3>\n",
    "Training neural networks is a time consuming task and it can take days or even weeks. Try not to leave this task to the last day. It is not necessary for you to use GPU for this task, but using it may drastically reduce the time required for you to complete this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a good amount of datasets in [torchvision](https://pytorch.org/docs/stable/torchvision/datasets.html), but in practice, chances are that you wouldn't find the dataset for your particular problem, so you should be capable of writing `DataLoader` for your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import PIL.Image as Image\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you are using the right device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_csv(r'data/cats_dogs/train.csv')\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('data/' + dt['path'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change class name \n",
    "class Your_class(Dataset):\n",
    "    \"\"\" Some documantation\"\"\"\n",
    "\n",
    "    def __init__(self,csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        \"\"\"\n",
    "        ### BEGIN Solution\n",
    "        # ... here you can load and initialize what you will need next\n",
    "        ### END Solution\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ### BEGIN Solution\n",
    "        # ... don't forget to augment your data for training, using the `transform` parameter of the constructor\n",
    "        ### END Solution\n",
    "        return img, torch.tensor(y)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        ### BEGIN Solution\n",
    "        # ... \n",
    "        ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the augmentation tranform and instantiate training and validation subsets of your `Dataset` and the correpsonding [`DataLoaders`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    ### BEGIN Solution\n",
    "    # ...\n",
    "    ### END Solution\n",
    "    ])\n",
    "\n",
    "### BEGIN Solution\n",
    "# dataset_train = \n",
    "# dataset_val = \n",
    "# train_loader = \n",
    "# val_loader = \n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that dataloader works as expected by observing one sample from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for X,y in train_loader:\n",
    "    print(X[0])\n",
    "    print(y[0])\n",
    "    plt.imshow(np.array(X[0,0,:,:]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your model below. You can use any layers that you want, but in general the structure of your model should be\n",
    "1. convolutional feature extractor, followed by\n",
    "2. fully-connected classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model_name(nn.Module):\n",
    "    def __init__(self, inp_ch=1, outp_ch=2):\n",
    "        super().__init__()\n",
    "        ### BEGIN Solution\n",
    "        # ...\n",
    "        ### END Solution\n",
    "       \n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        ### BEGIN Solution\n",
    "        # ...\n",
    "        ### END Solution\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send your model to GPU, if you have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model_name().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your loss function below, or use the predefined loss, suitable for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "# criterion = #\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try two different [optimizers](http://ruder.io/optimizing-gradient-descent/index.html) and choose one. For the optimizer of your choice, try two different sets of parameters (e.g learning rate). Explain both of your choices and back them with the learning performance of the network (see the rest of the task).\n",
    "\n",
    "In this parts of the task you may try more than two options, but, please, leave in your solution only the results for two different optimizers and two different sets of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "#optimizer = \n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may finally train you model. Don't forget to:\n",
    "1. monitor its training and validation performance *during training*, i.e plot the loss functions and prediction accuracy for train and validation sets, to make sure that your model doesn't learn complete nonsense; **do not** include tons of learning curves in your homework solution; (in real-life, you may find [`tensorboardX`](https://github.com/lanpa/tensorboardX) extremely useful for this task);\n",
    "2. visualize its training and validation performance *after training*, to demonstrate that you have accomplished the task;\n",
    "3. save the state of your model during the training, to use the best one at the end; you may find useful this [tutorial on saving and loading models](https://pytorch.org/tutorials/beginner/saving_loading_models.html);\n",
    "4. send the input and target data to the same device as your model.\n",
    "\n",
    "Your model should be able to show *at least 75% validation accuracy*.\n",
    "\n",
    "You may also find useful the following parts of documentation: [`Module.train`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.train), [`Module.eval`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval), [`Module.state_dict`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.state_dict), [`Module.load_state_dict`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.load_state_dict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9 (1 pt.): Bad activation function\n",
    "Using your conclusions from the <span style=\"color:red;\">Task 6</span>, choose the worst activation function and replace all activations in your model from the previous <span style=\"color:red;\">Task 8</span> with this one. Demonstrate the training and validation performance of this version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
