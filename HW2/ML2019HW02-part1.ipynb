{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Part 1 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem just right after the words **BEGIN SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "* The are problems with \\* mark - they are not obligatory. You can get **EXTRA POINTS** for solving them.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your theoretical derivations within such blocks:\n",
    "```markdown\n",
    "**BEGIN Solution**\n",
    "\n",
    "<!-- >>> your derivation here <<< -->\n",
    "\n",
    "**END Solution**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (1 pt.): Information criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that regression model is\n",
    "$$y = \\sum_{i=1}^k \\beta_i x_i + \\varepsilon,$$\n",
    "and $\\varepsilon$ is dictributed normally: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, $\\sigma^2$ is known.\n",
    "\n",
    "Prove that the model with highest Akaike information criterion is the model with smallest Mallow's $C_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "Akaike information criterion can be written as follows:\n",
    "$$AIC = \\ln(\\hat{L}) - 2k,$$\n",
    "where: $\\hat{L}$ is the maximized likelyhood, $k$ - number of estimated parameters in the model.\n",
    "\n",
    "Mallow $C_p$ can be written as follows:\n",
    "$$\n",
    "C_p = \\frac{SSE(\\beta)}{\\sigma^2} - N - 2k,\n",
    "$$\n",
    "The goal is to prove, that:\n",
    "$$\n",
    "\\text{argmax}_\\beta AIC = \\text{argmin}_\\beta C_p\n",
    "$$\n",
    "\n",
    "where $SSE(\\beta)$ is the error sum of squares for the model, $\\beta$ is the parameter vector, $N$ - sample size.\n",
    "For log-likelyhood for this simple regression model we can write (formula ideas are taken from https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf):\n",
    "$$\n",
    "\\ln(L) = - \\frac{N}{2}\\log2\\pi - N\\log \\sigma  - \\frac{1}{\\sigma^2}\\sum\\limits_{i = 1}^N (y_i - \\hat{y_i}(\\beta))^2 =   - \\frac{N}{2}\\log2\\pi - N\\log \\sigma  - \\frac{1}{\\sigma^2} SSE(\\beta)\n",
    "$$\n",
    "From this thing we see that $$\\max_\\beta(AIC) = 2\\max_\\beta(\\ln(L)) - 2k = 2\\max_\\beta(- \\frac{N}{2}\\log2\\pi - N\\log \\sigma  - \\frac{1}{\\sigma^2} SSE(\\beta)) - 2k$$\n",
    "From this equation it's obviously,that the needed to prove equation comes from this statement.\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting: gradient boosting, adaboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting and its theory\n",
    "\n",
    "Minimization of the loss function is an optimization task, and \"Gradient Boosting\"\n",
    "is one of the many methods to perform optimization. It shoould be noted that it\n",
    "uses *greedy* approach and therefore, like greedy algorithms in CS, may produce\n",
    "results that are not *globally* optimal.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & b_n(x) := \\text{the best base model from the family of the algorithms $\\mathcal{A}$} \\\\\n",
    "    & \\gamma_n(x) := \\text{scale or weight of the new model} \\\\\n",
    "    & a_N(x) = \\sum_{n=0}^N \\gamma_n b_n(x) := \\text{the final composite model}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Algorithm\n",
    "\n",
    "Consider a loss loss function $L(y, z)$ for target $y$ and prediction $z$, and let\n",
    "$(x_i, y_i)_{i=1}^l$ be our train dataset for a regression task. \n",
    "\n",
    "\n",
    "1. Initialize $a_0(x) = \\hat{z}$ with the *flat constant prediction*\n",
    "$\\hat{z} = \\arg\\min\\limits_{z \\in \\mathbb{R}} \\sum_{i=1}^l L(y_i, z)$;\n",
    "2. For $n$ from `1` to `n_boost_steps` do:\n",
    "    * Solve the current subporblem $G_n(b_n, \\gamma_n) \\to \\min\\limits_{b_{n}, \\gamma_n}$\n",
    "    where \n",
    "    $$ G_n(b, \\gamma) = \\sum_{i=1}^l L\\bigl(y_i, a_{n-1}(x_i) + \\gamma b(x_i)\\bigr) $$\n",
    "    with the following method:\n",
    "    \\begin{align}\n",
    "      & s_i = - \\frac{\\partial}{\\partial z} L(y_i, z) \\Big\\vert_{z=a_{n-1}(x_i)}\n",
    "          \\\\\n",
    "      & b_n(x) = \\arg\\min\\limits_{b\\in\\mathcal{A}}\\sum_{i=1}^l \\bigl(b(x_i) - s_i\\bigr)^2\n",
    "          \\\\\n",
    "      & \\gamma_n = \\arg\\min_\\gamma G_n(b_n, \\gamma)\n",
    "          \\\\\n",
    "      & a_n(x) = a_{n-1}(x) + \\gamma_n b_n(x)\n",
    "    \\end{align}\n",
    "3. return $a_N(x) = a_0(x) + \\sum_{n=1}^N \\gamma_n b_n(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (1 pt.)\n",
    "\n",
    "At the $n$-th step of Garient Boosting ($n \\geq 1$) we compute the \"residuals\"\n",
    "$(s_i)_{i=1}^l$ and learn $x\\mapsto b_n(x)$ with a regression algorithm $\\mathcal{A}$\n",
    "applied to the dataset $(x_i, s_i)_{i=1}^l$. For the next two tasks **assume\n",
    "that we have already perfomed these substeps**.\n",
    "\n",
    "Derive the **optimal value** of $\\gamma_n$ for *MSE* loss function $L(y, z) = \\tfrac12 (y - z)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "Recall, that $\\gamma_n = \\arg\\min_\\gamma G_n(b_n, \\gamma)$, where $G_n(b, \\gamma) = \\sum_{i=1}^l L\\bigl(y_i, a_{n-1}(x_i) + \\gamma b(x_i)\\bigr) = \\sum_{i=1}^l \\frac{1}{2}\\bigl(y_i - a_{n-1}(x_i) - \\gamma b(x_i)\\bigr)^2$. \n",
    "\n",
    "This is a convex function (as a combination of convex functions), so all we need is to write down normal equation:\n",
    "\n",
    "$$\\gamma G_n = -\\sum_{i=1}^l b(x_i)(y_i - a_{n-1}(x_i) - \\gamma b(x_i))=0$$\n",
    "\n",
    "which results in\n",
    "\n",
    "$$\n",
    "\\gamma_n = \\frac{\\sum_{i=1}^l b(x_i)(y_i-a_{n-1}(x_i))}{\\sum_{i=1}^l b(x_i)^2}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (1+1+1 pt.)\n",
    "\n",
    "Let $S = (x_i, y_i)_{i=1}^l$ be a sample for a classification task $y_i \\in \\{-1, +1\\}$.\n",
    "\n",
    "The **AdaBoost** algorithm can be regarded as a gradient boosting algorithm\n",
    "with the exponential loss $L(y,z) = e^{-y z}$. Consdier the state of **AdaBoost**\n",
    "at the $(T-1)$-step\n",
    "$$ G_{T-1}(b_T, \\gamma_T)\n",
    "    = \\sum_{i=1}^l L\\bigl(y_i, a_{n-1}(x_i) + \\gamma b(x_i)\\bigr)\n",
    "    = \\sum_{i=1}^l \\underbrace{\\exp(- y_i a_{T-1}(x_i))}_{w^{T-1}_i}\n",
    "        \\exp(- y_i \\gamma_T b_T(x_i))\n",
    "    \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.1 (1 pt.)\n",
    "\n",
    "Derive the next weights $w^T_i$ used in $G_T$ at the stage $T$ as a function\n",
    "of the learnt classifier $b_T$ and the current weights $w^{T-1}_i$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "For $a_T(x_i)$ we can write down:\n",
    "$a_T(x_i)= a_{T-1}(x_i)+\\gamma_{T}b_T(x_i)$, so \n",
    "\n",
    "$$\n",
    "w_i^{T}=\\text{exp}(-y_ia_T(x_i))=\\text{exp}\\left( -y_i\\left(a_{T-1}(x_i)+\\gamma_{T}b_T(x_i)\\right) \\right) = w_{i}^{T-1}\\text{exp}(-y_i \\gamma_{T}b_T(x_i)),\n",
    "$$\n",
    "where $\\gamma_T$ is from lectures:\n",
    "$$\\gamma_T = \\frac{1}{2}\\log \\frac{1 - N (b_T,\\hat{w}^T)}{N (b_T,\\hat{w}^T)},\n",
    "$$\n",
    "where $N (b_T,\\hat{w}_i^T) =\\sum_{i=1}^{m} \\hat{w}_i^T1\\{y_i b_T(x_i) \\leq 0\\}$ with denotation $\\hat{w}_i^T = \\frac{w_i^T}{\\sum_{i=1}^{l}w_i^T}$.\n",
    "\n",
    "The process is the following: we set the given $\\gamma_T$ into the equation for $w^T_i$ with the denotation given above $\\rightarrow$ we receive the $w_i^T$ as the function of the learnt classifier $b_T$ and the current weights $w^{T-1}_i$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.2 (1 pt.)\n",
    "\n",
    "Compute the ratio of weights $(w^T_i)_{i=1}^l$ between the normal and outlier\n",
    "samples in $S$. Propose a **formal definition of being an outlier**, and reflect\n",
    "on the value of *margin* for both.\n",
    "\n",
    "<span style=\"color:green\">**HINT**</span>: *margin* value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.3 (1 pt.)\n",
    "\n",
    "Conclude about **sensitivity** of Adaboost to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "Weights for outliers are bigger than for non-outliers, so, Adaboost is very sensetive to them.\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (2+1+2 pt.): Alternative objective functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem studies boosting-type algorithms defined with objective\n",
    "functions different from that of AdaBoost.We assume that the training\n",
    "data are given as m labeled examples $(x_{1},y_{1}),...,(x_{m},y_{m}) \\in X \\times \\{-1,+1\\}$.We further assume that $\\Phi$ is a strictly increasing convex and differentiable function over $\\mathbb{R}$ such that:$\\ \\forall x \\geqslant 0,\\Phi(x)\\geqslant 1 \\ and \\ \\forall x < 0,\\Phi(x) > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.1 (2 pt.)\n",
    "Consider the loss function $L(a) =\\sum_{i=1}^{m}\\Phi \\left( -y_{i}g(x_i) \\right)$ where $g$ is a linear combination of base classifiers, i.e., $g= \\sum_{t=1}^{T} a_t h_t$(as in\n",
    "AdaBoost). Derive a new boosting algorithm using the objective function $L$. In particular, characterize the best base classifier $h_u$ to select at each round of boosting if we use coordinate descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "First of all let's formulate the task in case of optimization problem:\n",
    "$$\n",
    "L(y;g(x)) = \\sum_{i=1}^{m}\\Phi \\left( -y_{i}g(x_i) \\right) \\rightarrow \\min\\limits_g \\rightarrow  |\\text{or after putting g} | \\rightarrow \n",
    "\\sum_{i=1}^{m}\\Phi \\left( -y_{i}g\\sum_{t=1}^{T} a_t h_t(x_i) \\right) \\rightarrow \\min_{a_t,h_t}\n",
    "$$\n",
    "\n",
    "On the $k$-step of the classifier using greedy algorithm to find the minimum for problem, stated above. So for our coordinate descent for $L(z)$ :\n",
    "$$\n",
    "a_{k+1} h_{k+1}(x_i) = g(x_i)^{k+1} - g(x_i)^k = - a_{k+1}\\frac{\\partial L(g(x_i)^k)}{\\partial (g(x_i)^k)}\\bigg|_{g(x_i)^k} = y_i a_{k+1} \\frac{\\partial \\Phi(t)}{\\partial t}\\bigg|_{t = g(x_i)^k} = a_{k+1} s_i\\\\\n",
    "$$\n",
    "So our best base classifier $h_u$ is to be trained on residuals $s_i$. The point is to step in direction of gradient descending, without the general loss of properties models (weak learners are used by models). After being trained by weak learner on residuals as the target, we are to optimize $a_{k+1}$ with resulting previously $h^{k+1}_i$ in the following way:\n",
    "$$a_{k+1} = arg\\min_a \\sum_{i=1}^{m}\\Phi \\left( -y_{i}(g_k(x_i) + ah_{k+1}(x_i)) \\right) =  \\arg\\min_a L_{k+1}(h_{k+1}, a)$$\n",
    "So the resulting sequence to be calculated and returned as the result of model work in the following way:\n",
    "\n",
    "$$g(x)^{n_b} = h_0(x) + \\sum_{n=1}^{n_b} a_n h_n(x)$$\n",
    "Finalizing this ideas can be represented in the algorithm, represented below, template for this algo is similar, that's why is taken from the top of this problem set for GB algo:\n",
    "\n",
    "\n",
    "1. Initialize $h_0(x) = \\hat{z}$ with the *flat constant prediction*\n",
    "$\\hat{z} = \\arg\\min\\limits_{z \\in \\mathbb{R}} \\sum_{i=1}^l L(y_i, z)$;\n",
    "2. For $n$ from `1` to `n_boost_steps` do:\n",
    "    * Solve the current subporblem $L(a_t, h_t) \\to \\min\\limits_{a_t, h_t}$\n",
    "    where \n",
    "    $$ L_{k+1}(h_{k+1}, a_{k+1}) = \\sum_{i=1}^l \\Phi\\bigl(y_i, g(x_i)^k + a_{k+1} h_{k+1}(x_i)\\bigr) $$\n",
    "    with the following method:\n",
    "    \\begin{align}\n",
    "      & s_i = - \\frac{\\partial}{\\partial z} L(y_i, z) \\Big\\vert_{z=a_{n-1}(x_i)} = y_i \\frac{\\partial \\Phi(t)}{\\partial t}\\bigg|_{t = g(x_i)^k}\n",
    "          \\\\\n",
    "      & h_{k+1}(x) = \\text{trained on residuals by weak learner}\n",
    "          \\\\\n",
    "      & a_{k+1} = \\arg\\min_a L_{k+1}(h_{k+1}, a)\n",
    "          \\\\\n",
    "      & g(x)^{k+1} = g(x)^{k} + a_{k+1} h_{k+1}(x)\n",
    "    \\end{align}\n",
    "3. return $g(x)^{n_b} = h_0(x) + \\sum_{n=1}^{n_b} a_n h_n(x)$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.2 (1 pt.)\n",
    "Consider the following functions:  \n",
    "\n",
    "1. zero-one loss $\\Phi_1(−u) = 1_{u\\leqslant0}$;  \n",
    "2. least squared loss $\\Phi_2(−u) = (1 − u)^2$;  \n",
    "3. SVM loss $\\Phi_3(−u) = \\max\\{0, 1 − u\\}$;  \n",
    "4. logistic loss $\\Phi_4(−u) = \\log(1 + e^{−u})$.  \n",
    "\n",
    "Which functions satisfy the assumptions on $\\Phi$ stated earlier in this\n",
    "problem?\n",
    "\n",
    "Compute the gradient for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "Functions from 1 to 3 don't satisfy the represented above assumptions on $\\Phi$, because all of them are not strictly increasing. The only loss function to satisfy asumptions on $\\Phi$, stated earlier is the logistic loss $\\Phi_4(−u) = \\log(1 + e^{−u})$, with it's gradient:\n",
    "$$\n",
    "\\text{grad} \\Phi_4(u) = \\frac{\\partial \\Phi_4(u)}{\\partial u} = \\frac{e^u}{1 + e^u}\n",
    "$$\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.3* (2 pt.)\n",
    "For each loss function satisfying the assumptions in Task 5 formualtion, derive the\n",
    "corresponding boosting algorithm. How do the algorithm(s) differ\n",
    "from AdaBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "The boosting algorithm is similar to the one, described in previous task, devoted to coordinate descent and to represented in the top of this file with the difference in residuals, now for $s_i  = - \\frac{\\partial}{\\partial z} L(y_i, z) \\Big\\vert_{z=a_{n-1}(x_i)} = y_i \\frac{\\partial \\Phi(t)}{\\partial t}\\bigg|_{t = g(x_i)^k} = y_i \\frac{e^{-y_i g(x_i)^k}}{1 + e^{-y_i g(x_i)^k}}\n",
    "$\n",
    "They are both variations of Gradient Boosting algorithm with different loss function is used, and different train sets (this residuals, AdaBoost on target values)& In AdaBoost logistic fucntion is used, which causes reweightning of sample on each iteration in opposite to our function. \n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. (1 pt.)\n",
    "Consider a two-layer network function of the form\n",
    "    \\begin{equation}\n",
    "    y_k(x, \\mathbf{w}) = \\sigma \\left ( \\sum_{j=1}^M w_{kj}^{(2)} \\sigma \\left(\\sum_{i=1}^D w_{ji}^{(1)}x_i + w_{j0}^{(1)}\\right)\n",
    "                               + w_{k0}^{(2)}\\right)\n",
    "    \\end{equation}\n",
    "in which the nonlinear activation functions are logistic sigmoid functions\n",
    "    \\begin{equation}\n",
    "    \\sigma(a) = (1 + \\exp(−a))^{-1}.\n",
    "    \\end{equation}\n",
    "Show that there exists an equivalent network, which computes exactly the same function but with hidden unit activation function given by hyperbolic tangent ${\\rm tanh}(a)$\n",
    "    \\begin{equation}\n",
    "    {\\rm tanh}(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}}.\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "    $$\n",
    "    \\text{tanh}(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}} \\rightarrow e^{-2a} = \\frac{1 - \\text{tanh}(a)}{1 + \\text{tanh}(a)}\n",
    "    $$\n",
    "    \n",
    "   From this we obtain dependency between logistic sigmoid function and hyperbolic tangent activation functions:\n",
    "   $$\n",
    "   \\sigma(2a) = \\frac{\\rm tanh(a) +1}{2} \\rightarrow \\sigma(a) = \\frac{\\rm tanh(\\frac{a}{2}) +1}{2} \n",
    "   $$\n",
    "   Using, this dependency, we can write for hidden layer activation function replacement (replaced sigmoid function by hyperbolic tangent):\n",
    "   $$\\sigma \\left(\\sum_{i=1}^D w_{ji}^{(1)}x_i + w_{j0}^{(1)}\\right) = 0.5{\\rm tanh} \\left(\\sum_{i=1}^D \\frac{w_{ji}^{(1)}}{2}x_i + \\frac{w_{j0}^{(1)}}{2}\\right) + \\frac{1}{2}$$\n",
    "   So an equivalent network, which computes exactly the same function but with hidden unit activation function given by hyperbolic tangent is represented as follows:\n",
    "   $$y_k(x, \\mathbf{w}) = \\sigma \\left ( \\sum_{j=1}^M (w_{new})_{kj}^{(2)} {\\rm tanh} \\left(\\sum_{i=1}^D (w_{new})_{ji}^{(1)}x_i + (w_{new})_{j0}^{(1)}\\right)\n",
    "                               + (w_{new})_{k0}^{(2)}\\right),$$\n",
    "                               \n",
    "                         \n",
    "  where\n",
    "  $$w^{(1)}_{ji new} = \\frac{w^{(1)}_{ji}}{2} \\ | \\ (w^{(2)}_{new})_{ij} = \\frac{w^{(2)}_{ij}}{2}, j \\neq 0 \\ | \\ (w^{(2)}_{new})_{k0} = w^{(2)}_{k0} + \\frac{\\sum_{j=1}^M w_{kj}^{(2)}}{2}$$\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6*. Data augmentation (2 pt.)\n",
    "One way to encourage invariance of a model w.r.t a set of transformations is to expand the training set using transformed versions of the original input patterns.\n",
    "Consider the framework for training with transformed data in the special case when the transformation is simply addition of random noise $x \\rightarrow x + \\xi$ where $\\xi$ has a Gaussian distribution with zero mean and unit variance. The error function for untransformed inputs can be written (in the infinite data set limit) in the form\n",
    "    \\begin{equation}\n",
    "    E = \\frac12 \\int \\int (y(\\mathbf{x}) - t)^2 p(t|\\mathbf{x}) p(\\mathbf{x}){\\rm d}\\mathbf{x} {\\rm d}t.\n",
    "    \\end{equation}\n",
    "If we now consider an infinite number of copies of each data point perturbed by the transformation, then the error function can be written as\n",
    "    \\begin{equation}\n",
    "    \\widetilde{E} = \\frac12 \\int\\int(y(x + \\xi) - t)^2p(t | \\mathbf{x})p(\\mathbf{x}) p(\\xi){\\rm d}\\mathbf{x}{\\rm d}t {\\rm d}\\xi\n",
    "    \\end{equation}\n",
    "Using Taylor series expansion of $y(\\mathbf{x} + \\xi)$ show that\n",
    "    \\begin{equation}\n",
    "    \\widetilde{E} \\simeq E + \\lambda \\Omega\n",
    "    \\end{equation}\n",
    "where $\\Omega$ is a regularizer which takes the form of Tikhonov regularizer\n",
    "    \\begin{equation}\n",
    "    \\Omega = \\frac12 \\int \\|\\nabla y(\\mathbf{x})\\|^2 p(\\mathbf{x}){\\rm d} \\mathbf{x}.\n",
    "    \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
